{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "75bc778ebd3343e49f032b813db1bfe6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f647adcc03b6451d8576d6ea91b0f243",
              "IPY_MODEL_4dc2a269af854e46a01c5b522e856770",
              "IPY_MODEL_ca9dd047ad8840b4bc819267e0af0b3e"
            ],
            "layout": "IPY_MODEL_73228da710454e799e0e3a24481c893f"
          }
        },
        "f647adcc03b6451d8576d6ea91b0f243": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e71a3f347eaf46d58d295a45b1171f52",
            "placeholder": "​",
            "style": "IPY_MODEL_07599d1b18cb44938bae73fbef01aafc",
            "value": "config.json: 100%"
          }
        },
        "4dc2a269af854e46a01c5b522e856770": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_73883b905317471d809774c4b7c7994b",
            "max": 665,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_87539df528df4b969ce949846b96a59b",
            "value": 665
          }
        },
        "ca9dd047ad8840b4bc819267e0af0b3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4a3eb20930804bbb9636123754542699",
            "placeholder": "​",
            "style": "IPY_MODEL_d7a826b18d6c4145a20c5fa24db1641c",
            "value": " 665/665 [00:00&lt;00:00, 73.6kB/s]"
          }
        },
        "73228da710454e799e0e3a24481c893f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e71a3f347eaf46d58d295a45b1171f52": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "07599d1b18cb44938bae73fbef01aafc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "73883b905317471d809774c4b7c7994b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "87539df528df4b969ce949846b96a59b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4a3eb20930804bbb9636123754542699": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d7a826b18d6c4145a20c5fa24db1641c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6fe6af2fce98465780d2f1b674d9aa2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_568d11f87e9e4774a227394fc37e6b41",
              "IPY_MODEL_1c7c2458f26e4939bd37a769f5705ae2",
              "IPY_MODEL_9bef37342c5b4bfd8374d9856debd3b3"
            ],
            "layout": "IPY_MODEL_c9c2fcb868b54010ba2941714c12cb64"
          }
        },
        "568d11f87e9e4774a227394fc37e6b41": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_897fe4d163bb4bfda0c200656101d68e",
            "placeholder": "​",
            "style": "IPY_MODEL_d8249ddcff1f479da9b5184967465b80",
            "value": "model.safetensors: 100%"
          }
        },
        "1c7c2458f26e4939bd37a769f5705ae2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_45b19d0e8abe48f09099d5143dc1523a",
            "max": 548105171,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0cc929a5fdfb472aa781a8607f65cfbf",
            "value": 548105171
          }
        },
        "9bef37342c5b4bfd8374d9856debd3b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fd29cf36783a45f781b96270aa22b778",
            "placeholder": "​",
            "style": "IPY_MODEL_e84448654f4f46ccb849202d55d535c4",
            "value": " 548M/548M [00:04&lt;00:00, 235MB/s]"
          }
        },
        "c9c2fcb868b54010ba2941714c12cb64": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "897fe4d163bb4bfda0c200656101d68e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d8249ddcff1f479da9b5184967465b80": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "45b19d0e8abe48f09099d5143dc1523a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0cc929a5fdfb472aa781a8607f65cfbf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fd29cf36783a45f781b96270aa22b778": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e84448654f4f46ccb849202d55d535c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "819f0c92d638419f9a559c1e1ef1ebb3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_953406aeeb864f8b9c6ce27e4864ca2e",
              "IPY_MODEL_0652a99c7d25412589595089f9894165",
              "IPY_MODEL_5bd8b30fcee2458ea27f6c801cacd02a"
            ],
            "layout": "IPY_MODEL_479743e710524f59ab065addac4be9a4"
          }
        },
        "953406aeeb864f8b9c6ce27e4864ca2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_158e338d02fb4bd987b11e562007f2f5",
            "placeholder": "​",
            "style": "IPY_MODEL_6c0a44c91ae345deb376301bd04c04c5",
            "value": "generation_config.json: 100%"
          }
        },
        "0652a99c7d25412589595089f9894165": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7e04e1fb26824e0b9e63dfc4fad5ae96",
            "max": 124,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0e5508717b7d4140958b3391a7b16703",
            "value": 124
          }
        },
        "5bd8b30fcee2458ea27f6c801cacd02a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ed99ea0113614ef2af381fdfa494382d",
            "placeholder": "​",
            "style": "IPY_MODEL_71b6c6e2cd5c4137a86372db4082f743",
            "value": " 124/124 [00:00&lt;00:00, 7.09kB/s]"
          }
        },
        "479743e710524f59ab065addac4be9a4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "158e338d02fb4bd987b11e562007f2f5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6c0a44c91ae345deb376301bd04c04c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7e04e1fb26824e0b9e63dfc4fad5ae96": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0e5508717b7d4140958b3391a7b16703": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ed99ea0113614ef2af381fdfa494382d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "71b6c6e2cd5c4137a86372db4082f743": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a67b61e6b5a84e30b441a778d18563e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_baaff16deb0a4e0483f247f4904fc21d",
              "IPY_MODEL_3f66cec0e52e481ca88977683520d93f",
              "IPY_MODEL_644c18256b984d03941346e81228fb4e"
            ],
            "layout": "IPY_MODEL_e278c88fed24441097386dcc0085911c"
          }
        },
        "baaff16deb0a4e0483f247f4904fc21d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_07d6dd4e8c9641d8a255bb3a82488ff8",
            "placeholder": "​",
            "style": "IPY_MODEL_88ea89194ce54682b274dcda17228b28",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "3f66cec0e52e481ca88977683520d93f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_528f7b3e29b74b5f99c2dc436634d157",
            "max": 26,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b26cb29348e7476a935f130589c7b0d7",
            "value": 26
          }
        },
        "644c18256b984d03941346e81228fb4e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_20ffea5b29ce4ed389c4a1cf0d6190dc",
            "placeholder": "​",
            "style": "IPY_MODEL_b5f78a93c7544210ac2be6626f440d2c",
            "value": " 26.0/26.0 [00:00&lt;00:00, 1.30kB/s]"
          }
        },
        "e278c88fed24441097386dcc0085911c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "07d6dd4e8c9641d8a255bb3a82488ff8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "88ea89194ce54682b274dcda17228b28": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "528f7b3e29b74b5f99c2dc436634d157": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b26cb29348e7476a935f130589c7b0d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "20ffea5b29ce4ed389c4a1cf0d6190dc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b5f78a93c7544210ac2be6626f440d2c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b879dc7a2eed450792f9df46506fa5bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_836976e217c9436eba8f8008d577a64f",
              "IPY_MODEL_435e4e762f1e43b1b6105af8d2336f4b",
              "IPY_MODEL_7b41b3f80f27442d969d7016fb76d847"
            ],
            "layout": "IPY_MODEL_17f9a93ea1bb4a8a8950d30ee1c62de3"
          }
        },
        "836976e217c9436eba8f8008d577a64f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9afe589fe26649fc94029be9080870df",
            "placeholder": "​",
            "style": "IPY_MODEL_e59f89514bf8437eb7b5ac83bac77142",
            "value": "vocab.json: 100%"
          }
        },
        "435e4e762f1e43b1b6105af8d2336f4b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0878fe551c944d23a7d73ffd39c3f2df",
            "max": 1042301,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_16332e9c70524ba4aa06049bc4d92a66",
            "value": 1042301
          }
        },
        "7b41b3f80f27442d969d7016fb76d847": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_018372c87d8e4f728ee7e8aad6a890f6",
            "placeholder": "​",
            "style": "IPY_MODEL_314c815144de40ea9be23ff10e033e05",
            "value": " 1.04M/1.04M [00:00&lt;00:00, 7.52MB/s]"
          }
        },
        "17f9a93ea1bb4a8a8950d30ee1c62de3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9afe589fe26649fc94029be9080870df": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e59f89514bf8437eb7b5ac83bac77142": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0878fe551c944d23a7d73ffd39c3f2df": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "16332e9c70524ba4aa06049bc4d92a66": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "018372c87d8e4f728ee7e8aad6a890f6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "314c815144de40ea9be23ff10e033e05": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a9cdc53ed4264809b0eb62db4be4c365": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_479c10fd5cd04e609ad946f60445feba",
              "IPY_MODEL_f749f8eab66a43ab8f859d0fc990b6a4",
              "IPY_MODEL_02a1394ca65c4582b684d7c9349ed98a"
            ],
            "layout": "IPY_MODEL_52cd11209a66458d99712c18755b52a2"
          }
        },
        "479c10fd5cd04e609ad946f60445feba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ee6eca44aa654f0b8a51b7779580b290",
            "placeholder": "​",
            "style": "IPY_MODEL_0a3a545977404765b50d21b540f2dc19",
            "value": "merges.txt: 100%"
          }
        },
        "f749f8eab66a43ab8f859d0fc990b6a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_def2d6435be44e5090f5a0772848b778",
            "max": 456318,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b3c8cf74ce4c44ceb80729af0ef3cd32",
            "value": 456318
          }
        },
        "02a1394ca65c4582b684d7c9349ed98a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9f16baa03d91464b92e104337e487bd9",
            "placeholder": "​",
            "style": "IPY_MODEL_481823e5c9a548f585e8f900704bad00",
            "value": " 456k/456k [00:00&lt;00:00, 8.98MB/s]"
          }
        },
        "52cd11209a66458d99712c18755b52a2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ee6eca44aa654f0b8a51b7779580b290": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0a3a545977404765b50d21b540f2dc19": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "def2d6435be44e5090f5a0772848b778": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b3c8cf74ce4c44ceb80729af0ef3cd32": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9f16baa03d91464b92e104337e487bd9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "481823e5c9a548f585e8f900704bad00": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4473152d58284c5f81ebffe9df73cf47": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4724aaad92a643499e0f26ae3015f50d",
              "IPY_MODEL_37f6c4b7a14947109b95f8d01bdb4d4c",
              "IPY_MODEL_564d18e684d14f8e93c1960503d192ec"
            ],
            "layout": "IPY_MODEL_9c1b7759f3e4473cb80398f17d26d882"
          }
        },
        "4724aaad92a643499e0f26ae3015f50d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5d5eb0b9a8bb4373aa4d77cee9faf889",
            "placeholder": "​",
            "style": "IPY_MODEL_547849fddf724e33b86fdcc9d5bfcac6",
            "value": "tokenizer.json: 100%"
          }
        },
        "37f6c4b7a14947109b95f8d01bdb4d4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7e02a6ec11cc43dc8a9f433f0c6dc3c5",
            "max": 1355256,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c3fae95e7d6c4332aa6595ea1c896b1c",
            "value": 1355256
          }
        },
        "564d18e684d14f8e93c1960503d192ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ef5e19299980497a8918cf9f72eff914",
            "placeholder": "​",
            "style": "IPY_MODEL_e871211e352f4471a994029a63761b21",
            "value": " 1.36M/1.36M [00:00&lt;00:00, 11.9MB/s]"
          }
        },
        "9c1b7759f3e4473cb80398f17d26d882": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5d5eb0b9a8bb4373aa4d77cee9faf889": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "547849fddf724e33b86fdcc9d5bfcac6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7e02a6ec11cc43dc8a9f433f0c6dc3c5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c3fae95e7d6c4332aa6595ea1c896b1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ef5e19299980497a8918cf9f72eff914": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e871211e352f4471a994029a63761b21": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 599
        },
        "id": "y_hmnXSdbevI",
        "outputId": "979d1a34-3e33-4568-82ab-c415d577864e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "plum-dispatch 2.6.0 requires beartype>=0.16.2, but you have beartype 0.14.1 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "pytensor 2.35.1 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "shap 0.50.0 requires numpy>=2, but you have numpy 1.26.4 which is incompatible.\n",
            "jaxlib 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "jax 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m✅ Laboratory Installed.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1169108559.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformer_lens\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHookedTransformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# --- STEP 2: LOAD THE SUBJECT (The Brain) ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformer_lens/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mhook_points\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mevals\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m from .past_key_value_caching import (\n\u001b[1;32m      5\u001b[0m     \u001b[0mHookedTransformerKeyValueCache\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformer_lens/hook_points.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformer_lens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSlice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSliceInput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformer_lens/utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marrow_dataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mhuggingface_hub\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconstants\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhf_hub_download\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/datasets/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0m__version__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"4.0.0\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0marrow_dataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0marrow_reader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mReadInstruction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mArrowBasedBuilder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBuilderConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDatasetBuilder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGeneratorBasedBuilder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfsspec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyarrow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyarrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m     ) from _err\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m from pandas._config import (\n\u001b[0m\u001b[1;32m     38\u001b[0m     \u001b[0mget_option\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mset_option\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/_config/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;34m\"warn_copy_on_write\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m ]\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdates\u001b[0m  \u001b[0;31m# pyright: ignore[reportUnusedImport]  # noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m from pandas._config.config import (\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/_config/config.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m from pandas._typing import (\n\u001b[0m\u001b[1;32m     69\u001b[0m     \u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/_typing.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGenerator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBitGenerator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRandomState\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numpy/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(attr)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__dir__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m         \u001b[0mpublic_symbols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglobals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'testing'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m         public_symbols -= {\n\u001b[1;32m    339\u001b[0m             \u001b[0;34m\"core\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"matrixlib\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numpy/random/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;31m# add these for module-freeze analysis (like PyInstaller)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_pickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_common\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_bounded_integers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numpy/random/_pickle.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmtrand\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRandomState\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_philox\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPhilox\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_pcg64\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPCG64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPCG64DXSM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_sfc64\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSFC64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mnumpy/random/mtrand.pyx\u001b[0m in \u001b[0;36minit numpy.random.mtrand\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
          ]
        }
      ],
      "source": [
        "# --- STEP 1: INSTALLATION (The \"Lab Equipment\") ---\n",
        "# We install TransformerLens, the standard tool for Mechanistic Interpretability.\n",
        "# We also install 'accelerate' and 'einops' for tensor operations.\n",
        "!pip install transformer_lens accelerate einops > /dev/null\n",
        "print(\"✅ Laboratory Installed.\")\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from transformer_lens import HookedTransformer\n",
        "\n",
        "# --- STEP 2: LOAD THE SUBJECT (The Brain) ---\n",
        "# We use GPT-2 Small. It's tiny (117M params) but sufficient to see structure.\n",
        "# It runs instantly on the Colab Free Tier (CPU or T4 GPU).\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"✅ Loading Model on: {device}...\")\n",
        "model = HookedTransformer.from_pretrained(\"gpt2-small\", device=device)\n",
        "print(\"✅ Model Active.\")\n",
        "\n",
        "# --- STEP 3: THE STIMULUS (The Experiment) ---\n",
        "# We create two inputs. Same topic (Health), different Rigor.\n",
        "\n",
        "# Input A: High Rigor (Mechanistic, Formal)\n",
        "# Note: We focus on specific keywords like 'randomized', 'control', 'p-value'.\n",
        "text_rigor = \"The double-blind study demonstrated a statistically significant reduction in inflammation (p < 0.05).\"\n",
        "\n",
        "# Input B: Low Rigor (Anecdotal, Casual)\n",
        "# Note: We use flags like 'my cousin', 'miracle', 'guaranteed'.\n",
        "text_sus   = \"My cousin tried this miracle tea and it totally cured his inflammation in like two days, guaranteed.\"\n",
        "\n",
        "# --- STEP 4: THE PROBE (The Scan) ---\n",
        "# We don't want the output text. We want the internal 'thought vector'.\n",
        "# We extract the 'Residual Stream' from the Middle Layer (Layer 6 of 12).\n",
        "# This is where the model processes \"concepts\" before turning them back into words.\n",
        "\n",
        "layer_idx = 6\n",
        "hook_name = f\"blocks.{layer_idx}.hook_resid_post\"\n",
        "\n",
        "print(f\"\\n🔬 Probing Layer {layer_idx}...\")\n",
        "\n",
        "# Run the Rigorous Text\n",
        "with torch.no_grad():\n",
        "    # run_with_cache returns the output AND a dictionary of all internal activations\n",
        "    _, cache_rigor = model.run_with_cache(text_rigor)\n",
        "    # We grab the vector for the FINAL token (the summary of the sentence so far)\n",
        "    vec_rigor = cache_rigor[hook_name][0, -1, :]\n",
        "\n",
        "# Run the Sus Text\n",
        "with torch.no_grad():\n",
        "    _, cache_sus = model.run_with_cache(text_sus)\n",
        "    vec_sus = cache_sus[hook_name][0, -1, :]\n",
        "\n",
        "# --- STEP 5: THE ANALYSIS (The Math) ---\n",
        "# Cosine Similarity measures the angle between two vectors.\n",
        "# 1.0 = Identical Direction (The model thinks these are the same concept)\n",
        "# 0.0 = Unrelated (Orthogonal)\n",
        "# -1.0 = Opposites\n",
        "\n",
        "similarity = F.cosine_similarity(vec_rigor, vec_sus, dim=0).item()\n",
        "\n",
        "print(\"-\" * 30)\n",
        "print(f\"High Rigor Input: '{text_rigor}'\")\n",
        "print(f\"Low Rigor Input:  '{text_sus}'\")\n",
        "print(\"-\" * 30)\n",
        "print(f\"STRUCTURAL SIMILARITY: {similarity:.4f}\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# Interpretation Logic\n",
        "if similarity < 0.90:\n",
        "    print(\"RESULT: SIGNIFICANT DIVERGENCE.\")\n",
        "    print(\"The model encodes these two inputs in distinctly different geometric spaces.\")\n",
        "    print(\"Hypothesis Supported: 'Rigor' acts as a separating vector.\")\n",
        "else:\n",
        "    print(\"RESULT: HIGH SIMILARITY.\")\n",
        "    print(\"The model views these primarily as 'Health Topics' and ignores the Rigor distinction.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- STEP 1: FIX DEPENDENCIES ---\n",
        "# We force upgrade numpy to version 2.0+ to match the expectations of newer libraries.\n",
        "# We also fix the 'beartype' conflict mentioned in your logs.\n",
        "!pip uninstall -y numpy\n",
        "!pip install \"numpy>=2.0.0\" \"beartype>=0.16.2\"\n",
        "\n",
        "# Re-install the main lab tools just to be safe\n",
        "!pip install transformer_lens accelerate einops\n",
        "\n",
        "print(\"✅ Libraries updated.\")\n",
        "print(\"⚠️ RESTARTING RUNTIME NOW... (This is normal!)\")\n",
        "\n",
        "# This command kills the process to force-reload the new libraries.\n",
        "# You will see a \"Session Crashed\" message. Just click \"Ok\" or wait for it to reconnect.\n",
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "zhE-fccTcAvM",
        "outputId": "4cb8ec53-f116-4ae1-a7ca-0dbb01bb03dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: numpy 1.26.4\n",
            "Uninstalling numpy-1.26.4:\n",
            "  Successfully uninstalled numpy-1.26.4\n",
            "Collecting numpy>=2.0.0\n",
            "  Downloading numpy-2.4.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting beartype>=0.16.2\n",
            "  Downloading beartype-0.22.9-py3-none-any.whl.metadata (37 kB)\n",
            "Downloading numpy-2.4.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m117.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading beartype-0.22.9-py3-none-any.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m71.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, beartype\n",
            "  Attempting uninstall: beartype\n",
            "    Found existing installation: beartype 0.14.1\n",
            "    Uninstalling beartype-0.14.1:\n",
            "      Successfully uninstalled beartype-0.14.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "transformer-lens 2.16.1 requires beartype<0.15.0,>=0.14.1, but you have beartype 0.22.9 which is incompatible.\n",
            "transformer-lens 2.16.1 requires numpy<2,>=1.26; python_version == \"3.12\", but you have numpy 2.4.1 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.4.1 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.4.1 which is incompatible.\n",
            "tensorflow 2.19.0 requires numpy<2.2.0,>=1.26.0, but you have numpy 2.4.1 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.4.1 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.4.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed beartype-0.22.9 numpy-2.4.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "106af68fe62e48658118265adddf6d83"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformer_lens in /usr/local/lib/python3.12/dist-packages (2.16.1)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.12.0)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.12/dist-packages (0.8.1)\n",
            "Collecting beartype<0.15.0,>=0.14.1 (from transformer_lens)\n",
            "  Using cached beartype-0.14.1-py3-none-any.whl.metadata (28 kB)\n",
            "Requirement already satisfied: better-abc<0.0.4,>=0.0.3 in /usr/local/lib/python3.12/dist-packages (from transformer_lens) (0.0.3)\n",
            "Requirement already satisfied: datasets>=2.7.1 in /usr/local/lib/python3.12/dist-packages (from transformer_lens) (4.0.0)\n",
            "Requirement already satisfied: fancy-einsum>=0.0.3 in /usr/local/lib/python3.12/dist-packages (from transformer_lens) (0.0.3)\n",
            "Requirement already satisfied: jaxtyping>=0.2.11 in /usr/local/lib/python3.12/dist-packages (from transformer_lens) (0.3.5)\n",
            "Collecting numpy<2,>=1.26 (from transformer_lens)\n",
            "  Using cached numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "Requirement already satisfied: pandas>=1.1.5 in /usr/local/lib/python3.12/dist-packages (from transformer_lens) (2.2.2)\n",
            "Requirement already satisfied: rich>=12.6.0 in /usr/local/lib/python3.12/dist-packages (from transformer_lens) (13.9.4)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (from transformer_lens) (0.2.1)\n",
            "Requirement already satisfied: torch>=2.6 in /usr/local/lib/python3.12/dist-packages (from transformer_lens) (2.9.0+cpu)\n",
            "Requirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.12/dist-packages (from transformer_lens) (4.67.1)\n",
            "Requirement already satisfied: transformers>=4.51 in /usr/local/lib/python3.12/dist-packages (from transformer_lens) (4.57.3)\n",
            "Requirement already satisfied: transformers-stream-generator<0.0.6,>=0.0.5 in /usr/local/lib/python3.12/dist-packages (from transformer_lens) (0.0.5)\n",
            "Requirement already satisfied: typeguard<5.0,>=4.2 in /usr/local/lib/python3.12/dist-packages (from transformer_lens) (4.4.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (from transformer_lens) (4.15.0)\n",
            "Requirement already satisfied: wandb>=0.13.5 in /usr/local/lib/python3.12/dist-packages (from transformer_lens) (0.23.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (25.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from accelerate) (6.0.3)\n",
            "Requirement already satisfied: huggingface_hub>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (0.36.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from accelerate) (0.7.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets>=2.7.1->transformer_lens) (3.20.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.7.1->transformer_lens) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.7.1->transformer_lens) (0.3.8)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.7.1->transformer_lens) (2.32.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets>=2.7.1->transformer_lens) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.7.1->transformer_lens) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.7.1->transformer_lens) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (1.2.0)\n",
            "Requirement already satisfied: wadler-lindig>=0.1.3 in /usr/local/lib/python3.12/dist-packages (from jaxtyping>=0.2.11->transformer_lens) (0.1.7)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.1.5->transformer_lens) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.1.5->transformer_lens) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.1.5->transformer_lens) (2025.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=12.6.0->transformer_lens) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=12.6.0->transformer_lens) (2.19.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.6->transformer_lens) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6->transformer_lens) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6->transformer_lens) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6->transformer_lens) (3.1.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.51->transformer_lens) (2025.11.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.51->transformer_lens) (0.22.1)\n",
            "Requirement already satisfied: click>=8.0.1 in /usr/local/lib/python3.12/dist-packages (from wandb>=0.13.5->transformer_lens) (8.3.1)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb>=0.13.5->transformer_lens) (3.1.45)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.12/dist-packages (from wandb>=0.13.5->transformer_lens) (4.5.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.12/dist-packages (from wandb>=0.13.5->transformer_lens) (5.29.5)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.12/dist-packages (from wandb>=0.13.5->transformer_lens) (2.12.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb>=0.13.5->transformer_lens) (2.47.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.7.1->transformer_lens) (3.13.2)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer_lens) (4.0.12)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=12.6.0->transformer_lens) (0.1.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb>=0.13.5->transformer_lens) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb>=0.13.5->transformer_lens) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb>=0.13.5->transformer_lens) (0.4.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=1.1.5->transformer_lens) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=2.7.1->transformer_lens) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=2.7.1->transformer_lens) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=2.7.1->transformer_lens) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=2.7.1->transformer_lens) (2025.11.12)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.6->transformer_lens) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.6->transformer_lens) (3.0.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.7.1->transformer_lens) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.7.1->transformer_lens) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.7.1->transformer_lens) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.7.1->transformer_lens) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.7.1->transformer_lens) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.7.1->transformer_lens) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.7.1->transformer_lens) (1.22.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer_lens) (5.0.2)\n",
            "Using cached beartype-0.14.1-py3-none-any.whl (739 kB)\n",
            "Using cached numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/cli/base_command.py\", line 179, in exc_logging_wrapper\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from transformer_lens import HookedTransformer\n",
        "\n",
        "# --- LOAD THE SUBJECT ---\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"✅ Loading Model on: {device}...\")\n",
        "model = HookedTransformer.from_pretrained(\"gpt2-small\", device=device)\n",
        "\n",
        "# --- THE STIMULUS ---\n",
        "# Input A: High Rigor\n",
        "text_rigor = \"The double-blind study demonstrated a statistically significant reduction in inflammation (p < 0.05).\"\n",
        "# Input B: Low Rigor\n",
        "text_sus   = \"My cousin tried this miracle tea and it totally cured his inflammation in like two days, guaranteed.\"\n",
        "\n",
        "# --- THE PROBE ---\n",
        "layer_idx = 6\n",
        "hook_name = f\"blocks.{layer_idx}.hook_resid_post\"\n",
        "\n",
        "print(f\"\\n🔬 Probing Layer {layer_idx}...\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    _, cache_rigor = model.run_with_cache(text_rigor)\n",
        "    vec_rigor = cache_rigor[hook_name][0, -1, :]\n",
        "\n",
        "with torch.no_grad():\n",
        "    _, cache_sus = model.run_with_cache(text_sus)\n",
        "    vec_sus = cache_sus[hook_name][0, -1, :]\n",
        "\n",
        "# --- THE ANALYSIS ---\n",
        "similarity = F.cosine_similarity(vec_rigor, vec_sus, dim=0).item()\n",
        "\n",
        "print(\"-\" * 30)\n",
        "print(f\"STRUCTURAL SIMILARITY: {similarity:.4f}\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "if similarity < 0.90:\n",
        "    print(\"RESULT: SIGNIFICANT DIVERGENCE (Success)\")\n",
        "else:\n",
        "    print(\"RESULT: HIGH SIMILARITY (Inconclusive)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547,
          "referenced_widgets": [
            "75bc778ebd3343e49f032b813db1bfe6",
            "f647adcc03b6451d8576d6ea91b0f243",
            "4dc2a269af854e46a01c5b522e856770",
            "ca9dd047ad8840b4bc819267e0af0b3e",
            "73228da710454e799e0e3a24481c893f",
            "e71a3f347eaf46d58d295a45b1171f52",
            "07599d1b18cb44938bae73fbef01aafc",
            "73883b905317471d809774c4b7c7994b",
            "87539df528df4b969ce949846b96a59b",
            "4a3eb20930804bbb9636123754542699",
            "d7a826b18d6c4145a20c5fa24db1641c",
            "6fe6af2fce98465780d2f1b674d9aa2e",
            "568d11f87e9e4774a227394fc37e6b41",
            "1c7c2458f26e4939bd37a769f5705ae2",
            "9bef37342c5b4bfd8374d9856debd3b3",
            "c9c2fcb868b54010ba2941714c12cb64",
            "897fe4d163bb4bfda0c200656101d68e",
            "d8249ddcff1f479da9b5184967465b80",
            "45b19d0e8abe48f09099d5143dc1523a",
            "0cc929a5fdfb472aa781a8607f65cfbf",
            "fd29cf36783a45f781b96270aa22b778",
            "e84448654f4f46ccb849202d55d535c4",
            "819f0c92d638419f9a559c1e1ef1ebb3",
            "953406aeeb864f8b9c6ce27e4864ca2e",
            "0652a99c7d25412589595089f9894165",
            "5bd8b30fcee2458ea27f6c801cacd02a",
            "479743e710524f59ab065addac4be9a4",
            "158e338d02fb4bd987b11e562007f2f5",
            "6c0a44c91ae345deb376301bd04c04c5",
            "7e04e1fb26824e0b9e63dfc4fad5ae96",
            "0e5508717b7d4140958b3391a7b16703",
            "ed99ea0113614ef2af381fdfa494382d",
            "71b6c6e2cd5c4137a86372db4082f743",
            "a67b61e6b5a84e30b441a778d18563e4",
            "baaff16deb0a4e0483f247f4904fc21d",
            "3f66cec0e52e481ca88977683520d93f",
            "644c18256b984d03941346e81228fb4e",
            "e278c88fed24441097386dcc0085911c",
            "07d6dd4e8c9641d8a255bb3a82488ff8",
            "88ea89194ce54682b274dcda17228b28",
            "528f7b3e29b74b5f99c2dc436634d157",
            "b26cb29348e7476a935f130589c7b0d7",
            "20ffea5b29ce4ed389c4a1cf0d6190dc",
            "b5f78a93c7544210ac2be6626f440d2c",
            "b879dc7a2eed450792f9df46506fa5bf",
            "836976e217c9436eba8f8008d577a64f",
            "435e4e762f1e43b1b6105af8d2336f4b",
            "7b41b3f80f27442d969d7016fb76d847",
            "17f9a93ea1bb4a8a8950d30ee1c62de3",
            "9afe589fe26649fc94029be9080870df",
            "e59f89514bf8437eb7b5ac83bac77142",
            "0878fe551c944d23a7d73ffd39c3f2df",
            "16332e9c70524ba4aa06049bc4d92a66",
            "018372c87d8e4f728ee7e8aad6a890f6",
            "314c815144de40ea9be23ff10e033e05",
            "a9cdc53ed4264809b0eb62db4be4c365",
            "479c10fd5cd04e609ad946f60445feba",
            "f749f8eab66a43ab8f859d0fc990b6a4",
            "02a1394ca65c4582b684d7c9349ed98a",
            "52cd11209a66458d99712c18755b52a2",
            "ee6eca44aa654f0b8a51b7779580b290",
            "0a3a545977404765b50d21b540f2dc19",
            "def2d6435be44e5090f5a0772848b778",
            "b3c8cf74ce4c44ceb80729af0ef3cd32",
            "9f16baa03d91464b92e104337e487bd9",
            "481823e5c9a548f585e8f900704bad00",
            "4473152d58284c5f81ebffe9df73cf47",
            "4724aaad92a643499e0f26ae3015f50d",
            "37f6c4b7a14947109b95f8d01bdb4d4c",
            "564d18e684d14f8e93c1960503d192ec",
            "9c1b7759f3e4473cb80398f17d26d882",
            "5d5eb0b9a8bb4373aa4d77cee9faf889",
            "547849fddf724e33b86fdcc9d5bfcac6",
            "7e02a6ec11cc43dc8a9f433f0c6dc3c5",
            "c3fae95e7d6c4332aa6595ea1c896b1c",
            "ef5e19299980497a8918cf9f72eff914",
            "e871211e352f4471a994029a63761b21"
          ]
        },
        "id": "xI6LxuYjcQqZ",
        "outputId": "042e048d-066d-416a-d24b-f6571837fdba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Loading Model on: cpu...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "75bc778ebd3343e49f032b813db1bfe6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6fe6af2fce98465780d2f1b674d9aa2e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "819f0c92d638419f9a559c1e1ef1ebb3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a67b61e6b5a84e30b441a778d18563e4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b879dc7a2eed450792f9df46506fa5bf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a9cdc53ed4264809b0eb62db4be4c365"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4473152d58284c5f81ebffe9df73cf47"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded pretrained model gpt2-small into HookedTransformer\n",
            "\n",
            "🔬 Probing Layer 6...\n",
            "------------------------------\n",
            "STRUCTURAL SIMILARITY: 0.8126\n",
            "------------------------------\n",
            "RESULT: SIGNIFICANT DIVERGENCE (Success)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- EXPERIMENT 2: THE \"STYLE CONTROL\" TEST ---\n",
        "# We keep the sentence structure identical.\n",
        "# We change ONLY the sample size integer.\n",
        "\n",
        "# Input A: Valid Stats\n",
        "text_valid = \"The clinical trial reported a significant p-value of 0.04 with a sample size of N=2000.\"\n",
        "\n",
        "# Input B: Garbage Stats (The \"P-Hacking\" Logic)\n",
        "text_invalid = \"The clinical trial reported a significant p-value of 0.04 with a sample size of N=3.\"\n",
        "\n",
        "print(f\"\\n🔬 CONTROLLING FOR STYLE...\")\n",
        "print(f\"Comparing N=2000 vs N=3\")\n",
        "\n",
        "# --- PROBE ---\n",
        "with torch.no_grad():\n",
        "    _, cache_valid = model.run_with_cache(text_valid)\n",
        "    # Note: We must grab the vector at the SAME token position to be fair.\n",
        "    # Both sentences are the same length, so '[-1]' works perfectly.\n",
        "    vec_valid = cache_valid[hook_name][0, -1, :]\n",
        "\n",
        "with torch.no_grad():\n",
        "    _, cache_invalid = model.run_with_cache(text_invalid)\n",
        "    vec_invalid = cache_invalid[hook_name][0, -1, :]\n",
        "\n",
        "# --- MEASURE ---\n",
        "similarity_control = F.cosine_similarity(vec_valid, vec_invalid, dim=0).item()\n",
        "\n",
        "print(\"-\" * 30)\n",
        "print(f\"LOGICAL SIMILARITY: {similarity_control:.4f}\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "if similarity_control > 0.98:\n",
        "    print(\"RESULT: MODEL BLINDNESS.\")\n",
        "    print(\"The model sees these as identical. It is looking at Tone, not Math.\")\n",
        "    print(\"It effectively 'believes' the bad stats because they sound formal.\")\n",
        "else:\n",
        "    print(\"RESULT: LOGICAL DETECTION.\")\n",
        "    print(\"The model distinguishes the numerical validity despite identical wording.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0SdlPCJqeCPb",
        "outputId": "84d8cd59-d21a-4e47-bacf-dc30058d043a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔬 CONTROLLING FOR STYLE...\n",
            "Comparing N=2000 vs N=3\n",
            "------------------------------\n",
            "LOGICAL SIMILARITY: 0.7908\n",
            "------------------------------\n",
            "RESULT: LOGICAL DETECTION.\n",
            "The model distinguishes the numerical validity despite identical wording.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- EXPERIMENT 3: THE \"SURPRISE\" TEST ---\n",
        "# We verify if the drop to 0.79 was due to \"Bad Science\" or just \"Different Number\".\n",
        "\n",
        "text_large  = \"The clinical trial reported a significant p-value of 0.04 with a sample size of N=2000.\"\n",
        "text_small  = \"The clinical trial reported a significant p-value of 0.04 with a sample size of N=3.\"\n",
        "text_medium = \"The clinical trial reported a significant p-value of 0.04 with a sample size of N=50.\"\n",
        "\n",
        "print(f\"\\n🔬 TRIANGULATING THE SIGNAL...\")\n",
        "\n",
        "# --- PROBE ---\n",
        "with torch.no_grad():\n",
        "    _, cache_large = model.run_with_cache(text_large)\n",
        "    vec_large = cache_large[hook_name][0, -1, :]\n",
        "\n",
        "    _, cache_small = model.run_with_cache(text_small)\n",
        "    vec_small = cache_small[hook_name][0, -1, :]\n",
        "\n",
        "    _, cache_medium = model.run_with_cache(text_medium)\n",
        "    vec_medium = cache_medium[hook_name][0, -1, :]\n",
        "\n",
        "# --- MEASURE ---\n",
        "# 1. The original \"Gap\" (2000 vs 3)\n",
        "sim_large_small = F.cosine_similarity(vec_large, vec_small, dim=0).item()\n",
        "\n",
        "# 2. The \"Valid vs Valid\" Gap (2000 vs 50)\n",
        "# If this is also 0.79, then the model doesn't care about validity, just the digits.\n",
        "sim_large_medium = F.cosine_similarity(vec_large, vec_medium, dim=0).item()\n",
        "\n",
        "# 3. The \"Small vs Tiny\" Gap (50 vs 3)\n",
        "sim_medium_small = F.cosine_similarity(vec_medium, vec_small, dim=0).item()\n",
        "\n",
        "print(\"-\" * 40)\n",
        "print(f\"1. Large (2000) vs Tiny (3):  {sim_large_small:.4f} (The previous result)\")\n",
        "print(f\"2. Large (2000) vs Med  (50):  {sim_large_medium:.4f}\")\n",
        "print(f\"3. Med   (50)   vs Tiny (3):  {sim_medium_small:.4f}\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# Interpretation\n",
        "if sim_large_medium > 0.90 and sim_large_small < 0.82:\n",
        "    print(\"CONCLUSION: VALIDITY DETECTED.\")\n",
        "    print(\"The model accepts N=50 as 'close' to N=2000, but rejects N=3.\")\n",
        "    print(\"It understands the threshold of 'Too Small'.\")\n",
        "elif sim_large_medium < 0.85:\n",
        "    print(\"CONCLUSION: NUMBER SENSITIVITY.\")\n",
        "    print(\"The model just treats all numbers as totally different vectors.\")\n",
        "    print(\"This is NOT a rigor check; it's just token arithmetic.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RGuJlNiaeUeC",
        "outputId": "11906ffd-351b-4869-fe55-470283fb1a35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔬 TRIANGULATING THE SIGNAL...\n",
            "----------------------------------------\n",
            "1. Large (2000) vs Tiny (3):  0.7908 (The previous result)\n",
            "2. Large (2000) vs Med  (50):  0.8819\n",
            "3. Med   (50)   vs Tiny (3):  0.9254\n",
            "----------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- EXPERIMENT 4: THE STYLE INJECTION ---\n",
        "# We take the \"Formal Science\" vector and FORCE it onto the \"Cousin\" input.\n",
        "\n",
        "# 1. Define the vectors again (from Exp 1)\n",
        "text_science = \"The double-blind study demonstrated a statistically significant reduction in inflammation.\"\n",
        "text_cousin  = \"My cousin tried this miracle tea and it totally cured his inflammation.\"\n",
        "\n",
        "# 2. Capture the \"Style Difference\"\n",
        "# We need the vector that points from \"Cousin\" -> \"Science\"\n",
        "with torch.no_grad():\n",
        "    _, cache_science = model.run_with_cache(text_science)\n",
        "    _, cache_cousin  = model.run_with_cache(text_cousin)\n",
        "\n",
        "    # We grab the average style of the middle layers (where tone lives)\n",
        "    style_vector = cache_science[hook_name][0, -1, :] - cache_cousin[hook_name][0, -1, :]\n",
        "\n",
        "# 3. The Hook Function (The Brain Surgery)\n",
        "# This function will run INSIDE the model while it generates text.\n",
        "def style_transfer_hook(resid_stream, hook):\n",
        "    # resid_stream shape: [batch, pos, d_model]\n",
        "    # We add the style vector to the LAST token position\n",
        "    # \"Strength\" controls how hard we push (Coefficient 5.0 is usually strong)\n",
        "    resid_stream[:, -1, :] += style_vector * 1.0\n",
        "    return resid_stream\n",
        "\n",
        "# 4. Generate: The Control (Natural Cousin)\n",
        "print(\"ORIGINAL COUSIN OUTPUT:\")\n",
        "print(model.generate(text_cousin, max_new_tokens=20, verbose=False))\n",
        "\n",
        "# 5. Generate: The Cyborg (Cousin + Science Injection)\n",
        "print(\"\\nINJECTED 'SCIENCE' OUTPUT:\")\n",
        "with model.hooks(fwd_hooks=[(hook_name, style_transfer_hook)]):\n",
        "    print(model.generate(text_cousin, max_new_tokens=20, verbose=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Srd_TcjeuLT",
        "outputId": "221553f8-ada8-473d-af2d-f4aea2fb0703"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ORIGINAL COUSIN OUTPUT:\n",
            "My cousin tried this miracle tea and it totally cured his inflammation. This method is no better for foreigners or for people suffering from diabetes.\n",
            "\n",
            "However, it is\n",
            "\n",
            "INJECTED 'SCIENCE' OUTPUT:\n",
            "My cousin tried this miracle tea and it totally cured his inflammation. He drank much less water, less caffeine and had zinc amulet, high calcium phosphate and we ate very\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- EXPERIMENT 5: THE SKEPTICAL OPTIMIZER ---\n",
        "# We verify if \"Bad Data\" creates a gradient that conflicts with \"Prior Knowledge\".\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# 1. Setup a tiny \"World Model\" (Simple Linear Network)\n",
        "# It learns a simple relationship: y = 2x (The \"Truth\")\n",
        "model = nn.Linear(1, 1, bias=False)\n",
        "# Initialize weight explicitly to something neutral\n",
        "with torch.no_grad():\n",
        "    model.weight.fill_(0.5)\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
        "\n",
        "# 2. Phase 1: Establish Consensus (Train on y=2x)\n",
        "print(\"🧠 PHASE 1: Learning The Truth (y=2x)...\")\n",
        "momentum_vector = torch.zeros_like(model.weight.grad, layout=torch.strided) if model.weight.grad is not None else torch.zeros_like(model.weight)\n",
        "\n",
        "# Train on 100 good samples\n",
        "for i in range(100):\n",
        "    x = torch.randn(1, 1)\n",
        "    y_truth = 2 * x # The Law of Physics\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    y_pred = model(x)\n",
        "    loss = F.mse_loss(y_pred, y_truth)\n",
        "    loss.backward()\n",
        "\n",
        "    # Update weights\n",
        "    optimizer.step()\n",
        "\n",
        "    # Update Momentum (The \"Consensus\")\n",
        "    # We use a simple moving average\n",
        "    if i == 0:\n",
        "        momentum_vector = model.weight.grad.clone()\n",
        "    else:\n",
        "        momentum_vector = 0.9 * momentum_vector + 0.1 * model.weight.grad\n",
        "\n",
        "print(f\"Current Belief (Weight): {model.weight.item():.4f} (Target: 2.0)\")\n",
        "\n",
        "# 3. Phase 2: The Anomaly (The \"Sus\" Data)\n",
        "# We introduce data that says y = -5x (The \"Lie\")\n",
        "x_bad = torch.tensor([[1.0]])\n",
        "y_bad = torch.tensor([[-5.0]]) # Radical contradiction\n",
        "\n",
        "optimizer.zero_grad()\n",
        "y_pred_bad = model(x_bad)\n",
        "loss_bad = F.mse_loss(y_pred_bad, y_bad)\n",
        "loss_bad.backward()\n",
        "\n",
        "gradient_sus = model.weight.grad.clone()\n",
        "\n",
        "# 4. Phase 3: The Measurement (Cosine Similarity)\n",
        "# Do the new data's demands align with the old data's direction?\n",
        "# Since it's 1D, we just check signs. In High-D, we use Cosine Sim.\n",
        "alignment = F.cosine_similarity(momentum_vector.flatten(), gradient_sus.flatten(), dim=0, eps=1e-8)\n",
        "\n",
        "print(\"\\n🔍 FORENSIC ANALYSIS:\")\n",
        "print(f\"Momentum (Truth) Vector: {momentum_vector.item():.4f}\")\n",
        "print(f\"New Data (Sus) Gradient: {gradient_sus.item():.4f}\")\n",
        "print(\"-\" * 30)\n",
        "print(f\"ALIGNMENT SCORE: {alignment.item():.4f}\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "if alignment < 0:\n",
        "    print(\"RESULT: REJECTION SUGGESTED.\")\n",
        "    print(\"The new data fights the established momentum.\")\n",
        "    print(\"A 'Skeptical Optimizer' would dampen this update.\")\n",
        "else:\n",
        "    print(\"RESULT: ACCEPTANCE.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D3ltH2lBocH4",
        "outputId": "53f6de97-0654-4a83-d782-a1557157f093"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧠 PHASE 1: Learning The Truth (y=2x)...\n",
            "Current Belief (Weight): 2.0000 (Target: 2.0)\n",
            "\n",
            "🔍 FORENSIC ANALYSIS:\n",
            "Momentum (Truth) Vector: -0.0001\n",
            "New Data (Sus) Gradient: 14.0000\n",
            "------------------------------\n",
            "ALIGNMENT SCORE: -1.0000\n",
            "------------------------------\n",
            "RESULT: REJECTION SUGGESTED.\n",
            "The new data fights the established momentum.\n",
            "A 'Skeptical Optimizer' would dampen this update.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- EXPERIMENT 6: HERESY VS NOVELTY (2D) [FIXED] ---\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# 1. THE WORLD MODEL (2D)\n",
        "class CircleModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(2, 16),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(16, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "model = CircleModel()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "# 2. PHASE 1: ESTABLISH CONSENSUS\n",
        "print(\"🧠 PHASE 1: Learning the Northern Hemisphere...\")\n",
        "\n",
        "momentum_vector = None\n",
        "\n",
        "for i in range(1000): # Increased to 1000 for better stability\n",
        "    # Generate points on the top half\n",
        "    theta = torch.rand(1) * 3.14159\n",
        "    x = torch.cos(theta)\n",
        "    y = torch.sin(theta)\n",
        "\n",
        "    # FIX: Ensure shape is [1, 2] (Batch size 1, 2 Features)\n",
        "    input_data = torch.cat([x, y], dim=0).unsqueeze(0)\n",
        "\n",
        "    target = torch.tensor([[1.0]]) # Target shape matches output [1, 1]\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    pred = model(input_data)\n",
        "    loss = F.mse_loss(pred, target)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Update Momentum\n",
        "    current_grad = torch.cat([p.grad.flatten() for p in model.parameters()])\n",
        "    if momentum_vector is None:\n",
        "        momentum_vector = current_grad\n",
        "    else:\n",
        "        momentum_vector = 0.9 * momentum_vector + 0.1 * current_grad\n",
        "\n",
        "print(\"✅ Consensus Established.\")\n",
        "\n",
        "# 3. PHASE 2: THE TESTS\n",
        "\n",
        "# CASE A: NOVELTY (The Southern Hemisphere)\n",
        "theta_new = torch.tensor([4.71]) # 270 degrees\n",
        "x_nov = torch.cos(theta_new)\n",
        "y_nov = torch.sin(theta_new)\n",
        "# FIX: Shape [1, 2]\n",
        "input_novelty = torch.cat([x_nov, y_nov], dim=0).unsqueeze(0)\n",
        "\n",
        "optimizer.zero_grad()\n",
        "pred_nov = model(input_novelty)\n",
        "loss_nov = F.mse_loss(pred_nov, torch.tensor([[1.0]]))\n",
        "loss_nov.backward()\n",
        "grad_novelty = torch.cat([p.grad.flatten() for p in model.parameters()])\n",
        "\n",
        "# CASE B: HERESY (The Origin - Violates Rule)\n",
        "# FIX: Shape [1, 2]\n",
        "input_heresy = torch.tensor([[0.0, 0.0]])\n",
        "\n",
        "optimizer.zero_grad()\n",
        "pred_her = model(input_heresy)\n",
        "loss_her = F.mse_loss(pred_her, torch.tensor([[1.0]])) # We lie and say it's valid\n",
        "loss_her.backward()\n",
        "grad_heresy = torch.cat([p.grad.flatten() for p in model.parameters()])\n",
        "\n",
        "# 4. THE MEASUREMENT\n",
        "align_novelty = F.cosine_similarity(momentum_vector, grad_novelty, dim=0)\n",
        "align_heresy  = F.cosine_similarity(momentum_vector, grad_heresy, dim=0)\n",
        "\n",
        "print(\"\\n🔍 FORENSIC ANALYSIS (2D Space):\")\n",
        "print(f\"Momentum vs. Novelty (New Valid Data): {align_novelty.item():.4f}\")\n",
        "print(f\"Momentum vs. Heresy  (New Bad Data):   {align_heresy.item():.4f}\")\n",
        "\n",
        "print(\"-\" * 30)\n",
        "if align_novelty > align_heresy:\n",
        "    print(\"RESULT: SUCCESS.\")\n",
        "    print(\"The model accepts Novelty (closer to consensus) but rejects Heresy.\")\n",
        "else:\n",
        "    print(\"RESULT: FAILURE.\")\n",
        "    print(\"The model cannot distinguish New from False.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UGzYjcfGpqVa",
        "outputId": "8604996c-6614-4adb-daf5-1a10e08f8357"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧠 PHASE 1: Learning the Northern Hemisphere...\n",
            "✅ Consensus Established.\n",
            "\n",
            "🔍 FORENSIC ANALYSIS (2D Space):\n",
            "Momentum vs. Novelty (New Valid Data): 0.3940\n",
            "Momentum vs. Heresy  (New Bad Data):   0.1333\n",
            "------------------------------\n",
            "RESULT: SUCCESS.\n",
            "The model accepts Novelty (closer to consensus) but rejects Heresy.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- EXPERIMENT 7: THE MANIFOLD DEFENSE (High-Dimensional Stress Test) ---\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# 1. CONFIGURATION\n",
        "DIM_INPUT = 128      # Simulating a small embedding space\n",
        "DIM_MANIFOLD = 10    # The \"Truth\" is a lower-dimensional structure (e.g., a specific topic)\n",
        "LEARNING_RATE = 0.01\n",
        "\n",
        "print(f\"🌌 INITIALIZING HIGH-DIMENSIONAL SPACE (D={DIM_INPUT})...\")\n",
        "\n",
        "# 2. DEFINE THE GROUND TRUTHS\n",
        "# We create a random basis matrix that defines the \"Valid Science\" subspace\n",
        "torch.manual_seed(42)\n",
        "truth_basis = torch.randn(DIM_MANIFOLD, DIM_INPUT)\n",
        "truth_basis = F.normalize(truth_basis, p=2, dim=1) # Orthonormal-ish basis\n",
        "\n",
        "# A separate basis for \"Pseudo-Science\" (Heresy)\n",
        "heresy_basis = torch.randn(DIM_MANIFOLD, DIM_INPUT)\n",
        "heresy_basis = F.normalize(heresy_basis, p=2, dim=1)\n",
        "\n",
        "# 3. THE MODEL (A Simple Probe)\n",
        "# Trying to learn: Is this vector \"Valid\"? (Target = 1.0)\n",
        "class ManifoldScanner(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(DIM_INPUT, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "model = ManifoldScanner()\n",
        "optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "# 4. PHASE 1: INDOCTRINATION (Training on the Truth Manifold)\n",
        "print(\"📚 PHASE 1: Learning the Scientific Manifold...\")\n",
        "\n",
        "momentum_vector = None\n",
        "num_batches = 1000\n",
        "\n",
        "for i in range(num_batches):\n",
        "    # Generate valid data: Linear combination of truth_basis vectors\n",
        "    # This simulates varied sentences about the SAME topic\n",
        "    coeffs = torch.randn(1, DIM_MANIFOLD)\n",
        "    input_data = torch.matmul(coeffs, truth_basis) # Shape [1, 128]\n",
        "\n",
        "    target = torch.tensor([[1.0]])\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    pred = model(input_data)\n",
        "    loss = F.mse_loss(pred, target)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Update Momentum (EMA)\n",
        "    current_grad = torch.cat([p.grad.flatten() for p in model.parameters()])\n",
        "    if momentum_vector is None:\n",
        "        momentum_vector = current_grad\n",
        "    else:\n",
        "        # Standard EMA\n",
        "        momentum_vector = 0.9 * momentum_vector + 0.1 * current_grad\n",
        "\n",
        "print(f\"✅ Training Complete. Momentum Vector Shape: {momentum_vector.shape}\")\n",
        "\n",
        "# 5. PHASE 2: THE HIGH-DIMENSIONAL JUDGMENT\n",
        "print(\"\\n⚖️  PHASE 2: JUDGMENT DAY\")\n",
        "\n",
        "# CASE A: NOVELTY (New Data from the Truth Manifold)\n",
        "# It's a combination of the basis we haven't seen, but it fits the math.\n",
        "coeffs_nov = torch.randn(1, DIM_MANIFOLD)\n",
        "input_nov = torch.matmul(coeffs_nov, truth_basis)\n",
        "\n",
        "optimizer.zero_grad()\n",
        "pred_nov = model(input_nov)\n",
        "loss_nov = F.mse_loss(pred_nov, torch.tensor([[1.0]]))\n",
        "loss_nov.backward()\n",
        "grad_novelty = torch.cat([p.grad.flatten() for p in model.parameters()])\n",
        "\n",
        "# CASE B: HERESY (Data from the Heresy Basis)\n",
        "# It mimics the shape (numbers look real), but the structural relationship is wrong.\n",
        "coeffs_her = torch.randn(1, DIM_MANIFOLD)\n",
        "input_her = torch.matmul(coeffs_her, heresy_basis) # Uses the WRONG basis\n",
        "\n",
        "optimizer.zero_grad()\n",
        "pred_her = model(input_her)\n",
        "# The Heretic claims this is valid (Target=1.0) - The \"Zinc Amulet\" lie\n",
        "loss_her = F.mse_loss(pred_her, torch.tensor([[1.0]]))\n",
        "loss_her.backward()\n",
        "grad_heresy = torch.cat([p.grad.flatten() for p in model.parameters()])\n",
        "\n",
        "# 6. THE MEASUREMENT\n",
        "align_novelty = F.cosine_similarity(momentum_vector, grad_novelty, dim=0)\n",
        "align_heresy  = F.cosine_similarity(momentum_vector, grad_heresy, dim=0)\n",
        "\n",
        "print(\"-\" * 40)\n",
        "print(f\"Gradient Alignment (Novelty/Truth): {align_novelty.item():.4f}\")\n",
        "print(f\"Gradient Alignment (Heresy/Lie):    {align_heresy.item():.4f}\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# Interpretation Logic\n",
        "threshold = 0.2 # Arbitrary barrier\n",
        "if align_novelty > align_heresy + 0.1: # Significant gap\n",
        "    print(\"RESULT: ROBUST.\")\n",
        "    print(\"Despite high dimensions, the momentum cleanly separates the manifolds.\")\n",
        "    print(\"The 'Skeptical Optimizer' survives the Curse of Dimensionality.\")\n",
        "else:\n",
        "    print(\"RESULT: COLLAPSE.\")\n",
        "    print(\"The high-dimensional noise drowned out the signal.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B8Kh-hkEq63-",
        "outputId": "ede7e392-7984-4bba-adb0-a40b09f41072"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🌌 INITIALIZING HIGH-DIMENSIONAL SPACE (D=128)...\n",
            "📚 PHASE 1: Learning the Scientific Manifold...\n",
            "✅ Training Complete. Momentum Vector Shape: torch.Size([4161])\n",
            "\n",
            "⚖️  PHASE 2: JUDGMENT DAY\n",
            "----------------------------------------\n",
            "Gradient Alignment (Novelty/Truth): -0.4172\n",
            "Gradient Alignment (Heresy/Lie):    -0.2958\n",
            "----------------------------------------\n",
            "RESULT: COLLAPSE.\n",
            "The high-dimensional noise drowned out the signal.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- EXPERIMENT 7.1: STABILIZED SKEPTIC (Batch Normalization Fix) ---\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# 1. CONFIGURATION\n",
        "DIM_INPUT = 128\n",
        "DIM_MANIFOLD = 10\n",
        "BATCH_SIZE = 32  # <--- THE CRITICAL FIX (Was 1)\n",
        "LEARNING_RATE = 0.01\n",
        "\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# 2. DEFINITIONS (Same as before)\n",
        "truth_basis = F.normalize(torch.randn(DIM_MANIFOLD, DIM_INPUT), p=2, dim=1)\n",
        "heresy_basis = F.normalize(torch.randn(DIM_MANIFOLD, DIM_INPUT), p=2, dim=1)\n",
        "\n",
        "class ManifoldScanner(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(DIM_INPUT, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "model = ManifoldScanner()\n",
        "optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "# 3. PHASE 1: STABILIZED INDOCTRINATION\n",
        "print(f\"📚 PHASE 1: Learning with Batch Size {BATCH_SIZE}...\")\n",
        "\n",
        "momentum_vector = None\n",
        "num_batches = 500 # Fewer steps, but more data per step\n",
        "\n",
        "for i in range(num_batches):\n",
        "    # Generate BATCH of valid data\n",
        "    coeffs = torch.randn(BATCH_SIZE, DIM_MANIFOLD)\n",
        "    input_data = torch.matmul(coeffs, truth_basis) # Shape [32, 128]\n",
        "\n",
        "    # Target is all 1.0s\n",
        "    target = torch.ones(BATCH_SIZE, 1)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    pred = model(input_data)\n",
        "    loss = F.mse_loss(pred, target)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Update Momentum (EMA)\n",
        "    # We detach to prevent graph leaks, though strict SGD doesn't need it\n",
        "    current_grad = torch.cat([p.grad.flatten() for p in model.parameters()])\n",
        "\n",
        "    if momentum_vector is None:\n",
        "        momentum_vector = current_grad\n",
        "    else:\n",
        "        momentum_vector = 0.9 * momentum_vector + 0.1 * current_grad\n",
        "\n",
        "print(\"✅ Consensus Established.\")\n",
        "\n",
        "# 4. PHASE 2: THE JUDGMENT (Batch vs Batch)\n",
        "print(\"\\n⚖️  PHASE 2: BATCH JUDGMENT\")\n",
        "\n",
        "# CASE A: NOVELTY BATCH (New Valid Data)\n",
        "coeffs_nov = torch.randn(BATCH_SIZE, DIM_MANIFOLD)\n",
        "input_nov = torch.matmul(coeffs_nov, truth_basis)\n",
        "\n",
        "optimizer.zero_grad()\n",
        "pred_nov = model(input_nov)\n",
        "loss_nov = F.mse_loss(pred_nov, torch.ones(BATCH_SIZE, 1))\n",
        "loss_nov.backward()\n",
        "grad_novelty = torch.cat([p.grad.flatten() for p in model.parameters()])\n",
        "\n",
        "# CASE B: HERESY BATCH (New Invalid Data)\n",
        "coeffs_her = torch.randn(BATCH_SIZE, DIM_MANIFOLD)\n",
        "input_her = torch.matmul(coeffs_her, heresy_basis) # Wrong Basis\n",
        "\n",
        "optimizer.zero_grad()\n",
        "pred_her = model(input_her)\n",
        "loss_her = F.mse_loss(pred_her, torch.ones(BATCH_SIZE, 1)) # The Lie\n",
        "loss_her.backward()\n",
        "grad_heresy = torch.cat([p.grad.flatten() for p in model.parameters()])\n",
        "\n",
        "# 5. THE MEASUREMENT\n",
        "align_novelty = F.cosine_similarity(momentum_vector, grad_novelty, dim=0)\n",
        "align_heresy  = F.cosine_similarity(momentum_vector, grad_heresy, dim=0)\n",
        "\n",
        "print(\"-\" * 40)\n",
        "print(f\"Gradient Alignment (Novelty): {align_novelty.item():.4f}\")\n",
        "print(f\"Gradient Alignment (Heresy):  {align_heresy.item():.4f}\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "if align_novelty > align_heresy:\n",
        "    print(\"RESULT: SUCCESS.\")\n",
        "    if align_novelty > 0:\n",
        "        print(\"Note: Positive alignment achieved (Oscillation dampened).\")\n",
        "else:\n",
        "    print(\"RESULT: PERSISTENT FAILURE.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8rG_UpO7rIB1",
        "outputId": "cbd5cfe0-7315-477f-b554-fd996b6ae189"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📚 PHASE 1: Learning with Batch Size 32...\n",
            "✅ Consensus Established.\n",
            "\n",
            "⚖️  PHASE 2: BATCH JUDGMENT\n",
            "----------------------------------------\n",
            "Gradient Alignment (Novelty): -0.0132\n",
            "Gradient Alignment (Heresy):  0.1008\n",
            "----------------------------------------\n",
            "RESULT: PERSISTENT FAILURE.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- EXPERIMENT 7.2: THE RED QUEEN (Weight Decay + Style Bias) ---\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# 1. CONFIGURATION\n",
        "DIM_INPUT = 128\n",
        "DIM_MANIFOLD = 10\n",
        "BATCH_SIZE = 32\n",
        "LEARNING_RATE = 0.01\n",
        "WEIGHT_DECAY = 1e-2  # <--- FIX 1: The Red Queen (Forces active maintenance)\n",
        "\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# 2. DEFINITIONS\n",
        "# The \"Style\" (e.g., the word 'p-value' or 'rigor')\n",
        "# This gives the Truth Manifold a specific NON-ZERO mean direction.\n",
        "style_vector = torch.randn(1, DIM_INPUT) * 2.0  # Strong signal\n",
        "\n",
        "truth_basis = F.normalize(torch.randn(DIM_MANIFOLD, DIM_INPUT), p=2, dim=1)\n",
        "heresy_basis = F.normalize(torch.randn(DIM_MANIFOLD, DIM_INPUT), p=2, dim=1)\n",
        "\n",
        "class ManifoldScanner(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # We use a Bias=False layer to strictly test vector alignment\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(DIM_INPUT, 32, bias=False),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, 1, bias=False)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "model = ManifoldScanner()\n",
        "# Add Weight Decay to the optimizer\n",
        "optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
        "\n",
        "# 3. PHASE 1: INDOCTRINATION (With Style)\n",
        "print(f\"📚 PHASE 1: Learning the 'Scientific Style'...\")\n",
        "\n",
        "momentum_vector = None\n",
        "num_batches = 500\n",
        "\n",
        "for i in range(num_batches):\n",
        "    coeffs = torch.randn(BATCH_SIZE, DIM_MANIFOLD)\n",
        "    # FIX 2: Add the Style Vector to the inputs\n",
        "    # Valid Data = Random Facts (Basis) + Scientific Tone (Style)\n",
        "    raw_signal = torch.matmul(coeffs, truth_basis)\n",
        "    input_data = raw_signal + style_vector\n",
        "\n",
        "    target = torch.ones(BATCH_SIZE, 1)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    pred = model(input_data)\n",
        "    loss = F.mse_loss(pred, target)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Capture Momentum\n",
        "    current_grad = torch.cat([p.grad.flatten() for p in model.parameters()])\n",
        "    if momentum_vector is None:\n",
        "        momentum_vector = current_grad\n",
        "    else:\n",
        "        momentum_vector = 0.9 * momentum_vector + 0.1 * current_grad\n",
        "\n",
        "print(\"✅ Consensus Established.\")\n",
        "\n",
        "# 4. PHASE 2: THE JUDGMENT\n",
        "print(\"\\n⚖️  PHASE 2: RED QUEEN JUDGMENT\")\n",
        "\n",
        "# CASE A: NOVELTY (New Valid Facts + Correct Style)\n",
        "coeffs_nov = torch.randn(BATCH_SIZE, DIM_MANIFOLD)\n",
        "input_nov = torch.matmul(coeffs_nov, truth_basis) + style_vector # Has the 'Amulet'\n",
        "\n",
        "optimizer.zero_grad()\n",
        "pred_nov = model(input_nov)\n",
        "loss_nov = F.mse_loss(pred_nov, torch.ones(BATCH_SIZE, 1))\n",
        "loss_nov.backward()\n",
        "grad_novelty = torch.cat([p.grad.flatten() for p in model.parameters()])\n",
        "\n",
        "# CASE B: HERESY (Wrong Facts + Wrong/No Style)\n",
        "coeffs_her = torch.randn(BATCH_SIZE, DIM_MANIFOLD)\n",
        "input_her = torch.matmul(coeffs_her, heresy_basis) # Missing the Style Vector!\n",
        "\n",
        "optimizer.zero_grad()\n",
        "pred_her = model(input_her)\n",
        "loss_her = F.mse_loss(pred_her, torch.ones(BATCH_SIZE, 1))\n",
        "loss_her.backward()\n",
        "grad_heresy = torch.cat([p.grad.flatten() for p in model.parameters()])\n",
        "\n",
        "# 5. THE MEASUREMENT\n",
        "align_novelty = F.cosine_similarity(momentum_vector, grad_novelty, dim=0)\n",
        "align_heresy  = F.cosine_similarity(momentum_vector, grad_heresy, dim=0)\n",
        "\n",
        "print(\"-\" * 40)\n",
        "print(f\"Gradient Alignment (Novelty): {align_novelty.item():.4f}\")\n",
        "print(f\"Gradient Alignment (Heresy):  {align_heresy.item():.4f}\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "if align_novelty > 0.5 and align_heresy < 0.2:\n",
        "    print(\"RESULT: SUCCESS.\")\n",
        "    print(\"The 'Red Queen' (Weight Decay) forced the model to keep the 'Truth Vector' active.\")\n",
        "else:\n",
        "    print(\"RESULT: FAILURE.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E0rU8yworbz_",
        "outputId": "ee388ec3-be02-4d85-b352-cd528643cc0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📚 PHASE 1: Learning the 'Scientific Style'...\n",
            "✅ Consensus Established.\n",
            "\n",
            "⚖️  PHASE 2: RED QUEEN JUDGMENT\n",
            "----------------------------------------\n",
            "Gradient Alignment (Novelty): 0.3464\n",
            "Gradient Alignment (Heresy):  0.0138\n",
            "----------------------------------------\n",
            "RESULT: FAILURE.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- EXPERIMENT 8: THE CURRICULUM BOOTSTRAP (Math -> Science) ---\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# 1. CONFIGURATION\n",
        "DIM_INPUT = 64      # Embedding size\n",
        "BATCH_SIZE = 32\n",
        "LEARNING_RATE = 0.01\n",
        "WEIGHT_DECAY = 1e-2 # The Red Queen (Essential!)\n",
        "\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# 2. THE WORLD\n",
        "# The \"Logic\" is strictly: Output = Input[0] + Input[1]\n",
        "# The other 62 dimensions are \"Style/Noise\"\n",
        "\n",
        "class LogicModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # A simple linear probe is enough to test alignment\n",
        "        self.net = nn.Linear(DIM_INPUT, 1, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "model = LogicModel()\n",
        "optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
        "\n",
        "# 3. PHASE 1: MATH TEXTBOOKS (Clean Logic, No Noise)\n",
        "print(\"📘 PHASE 1: Bootstrapping with 'Math Textbooks'...\")\n",
        "print(\"(Training strictly on x1 + x2 = y, with 0 noise)\")\n",
        "\n",
        "momentum_vector = None\n",
        "num_batches = 500\n",
        "\n",
        "for i in range(num_batches):\n",
        "    # Data: First 2 dims are numbers, rest are Zeros (Clean Math)\n",
        "    data = torch.randn(BATCH_SIZE, 2)\n",
        "    noise = torch.zeros(BATCH_SIZE, DIM_INPUT - 2)\n",
        "    input_data = torch.cat([data, noise], dim=1)\n",
        "\n",
        "    # Target: The Sum of the first two numbers\n",
        "    target = (data[:, 0] + data[:, 1]).unsqueeze(1)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    pred = model(input_data)\n",
        "    loss = F.mse_loss(pred, target)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Update Momentum\n",
        "    current_grad = torch.cat([p.grad.flatten() for p in model.parameters()])\n",
        "    if momentum_vector is None:\n",
        "        momentum_vector = current_grad\n",
        "    else:\n",
        "        momentum_vector = 0.9 * momentum_vector + 0.1 * current_grad\n",
        "\n",
        "print(\"✅ Logic Momentum Established.\")\n",
        "\n",
        "# 4. PHASE 2: THE INTERNET (Noisy Data)\n",
        "print(\"\\n🌍 PHASE 2: FILTERING THE INTERNET\")\n",
        "\n",
        "# Define the \"Zinc Amulet\" (Scientific Style)\n",
        "# This is random noise that 'looks' complex but is irrelevant to the logic.\n",
        "style_noise = torch.randn(BATCH_SIZE, DIM_INPUT - 2) * 2.0\n",
        "\n",
        "# CASE A: VALID SCIENCE (Messy but True)\n",
        "# It has the Logic (x1+x2) AND the Style (Noise)\n",
        "data_valid = torch.randn(BATCH_SIZE, 2)\n",
        "input_valid = torch.cat([data_valid, style_noise], dim=1) # Valid + Style\n",
        "target_valid = (data_valid[:, 0] + data_valid[:, 1]).unsqueeze(1) # Correct Answer\n",
        "\n",
        "optimizer.zero_grad()\n",
        "pred_v = model(input_valid)\n",
        "loss_v = F.mse_loss(pred_v, target_valid)\n",
        "loss_v.backward()\n",
        "grad_valid = torch.cat([p.grad.flatten() for p in model.parameters()])\n",
        "\n",
        "# CASE B: SLOP / BAD SCIENCE (Messy and False)\n",
        "# It has the Style (looks like science) but the Answer is WRONG.\n",
        "data_slop = torch.randn(BATCH_SIZE, 2)\n",
        "input_slop = torch.cat([data_slop, style_noise], dim=1) # Slop + Style\n",
        "# THE LIE: The target is random, not the sum.\n",
        "# Simulating \"N=3, p<0.05\" (The numbers don't add up)\n",
        "target_slop = torch.randn(BATCH_SIZE, 1)\n",
        "\n",
        "optimizer.zero_grad()\n",
        "pred_s = model(input_slop)\n",
        "# The Slop Author claims this is the truth (we compute loss against their lie)\n",
        "loss_s = F.mse_loss(pred_s, target_slop)\n",
        "loss_s.backward()\n",
        "grad_slop = torch.cat([p.grad.flatten() for p in model.parameters()])\n",
        "\n",
        "# 5. THE MEASUREMENT\n",
        "align_valid = F.cosine_similarity(momentum_vector, grad_valid, dim=0)\n",
        "align_slop  = F.cosine_similarity(momentum_vector, grad_slop, dim=0)\n",
        "\n",
        "print(\"-\" * 40)\n",
        "print(f\"Gradient Alignment (Valid Science): {align_valid.item():.4f}\")\n",
        "print(f\"Gradient Alignment (Slop/Bad Logic): {align_slop.item():.4f}\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "if align_valid > align_slop + 0.5: # Strict threshold\n",
        "    print(\"RESULT: SUCCESS.\")\n",
        "    print(\"The model used its 'Math Intuition' to accept Valid Science and reject Slop.\")\n",
        "    print(\"Even though both inputs had the same 'Style' (Noise), only logic aligned with Momentum.\")\n",
        "else:\n",
        "    print(\"RESULT: FAILURE.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fwBprTd6sK7g",
        "outputId": "ce945e6a-7ce0-45a8-bf1a-0672da2ec2db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📘 PHASE 1: Bootstrapping with 'Math Textbooks'...\n",
            "(Training strictly on x1 + x2 = y, with 0 noise)\n",
            "✅ Logic Momentum Established.\n",
            "\n",
            "🌍 PHASE 2: FILTERING THE INTERNET\n",
            "----------------------------------------\n",
            "Gradient Alignment (Valid Science): 0.0505\n",
            "Gradient Alignment (Slop/Bad Logic): -0.1858\n",
            "----------------------------------------\n",
            "RESULT: FAILURE.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- EXPERIMENT 9: CAUSAL PATHWAY VERIFICATION ---\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# 1. CONFIGURATION\n",
        "DIM_INPUT = 100     # 100 Potential \"Causes\"\n",
        "BATCH_SIZE = 64\n",
        "LEARNING_RATE = 0.05 # Aggressive learning to ensure gradients exist\n",
        "WEIGHT_DECAY = 0.01\n",
        "\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# 2. THE MODEL (Linear Probe)\n",
        "class CausalScanner(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # No bias, strictly testing weight alignment\n",
        "        self.net = nn.Linear(DIM_INPUT, 1, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "model = CausalScanner()\n",
        "optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
        "\n",
        "# 3. PHASE 1: ESTABLISHING CAUSALITY\n",
        "# The Truth: Only Input[0] matters. y = 3.0 * x[0]\n",
        "print(\"🔬 PHASE 1: Isolating the Causal Variable (Dim 0)...\")\n",
        "\n",
        "momentum_vector = None\n",
        "num_batches = 500\n",
        "\n",
        "for i in range(num_batches):\n",
        "    # Inputs: Random noise across all 100 dimensions\n",
        "    input_data = torch.randn(BATCH_SIZE, DIM_INPUT)\n",
        "\n",
        "    # Target: Driven STRICTLY by Index 0 (The \"Virus\")\n",
        "    target = (3.0 * input_data[:, 0]).unsqueeze(1)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    pred = model(input_data)\n",
        "    loss = F.mse_loss(pred, target)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Capture Momentum\n",
        "    current_grad = torch.cat([p.grad.flatten() for p in model.parameters()])\n",
        "    if momentum_vector is None:\n",
        "        momentum_vector = current_grad\n",
        "    else:\n",
        "        momentum_vector = 0.9 * momentum_vector + 0.1 * current_grad\n",
        "\n",
        "print(\"✅ Causal Link Established.\")\n",
        "\n",
        "# 4. PHASE 2: THE \"ZINC AMULET\" TEST\n",
        "print(\"\\n🔮 PHASE 2: DETECTING FALSE CAUSALITY\")\n",
        "\n",
        "# CASE A: NOVELTY (The Mutation)\n",
        "# The \"Virus\" (Dim 0) is still the cause, but the data is weird/stronger.\n",
        "# The model MUST update weights to handle this, so Gradient != 0.\n",
        "input_valid = torch.randn(BATCH_SIZE, DIM_INPUT)\n",
        "target_valid = (3.5 * input_valid[:, 0]).unsqueeze(1) # Coefficient changed slightly (Mutation)\n",
        "\n",
        "optimizer.zero_grad()\n",
        "pred_v = model(input_valid)\n",
        "loss_v = F.mse_loss(pred_v, target_valid)\n",
        "loss_v.backward()\n",
        "grad_valid = torch.cat([p.grad.flatten() for p in model.parameters()])\n",
        "\n",
        "# CASE B: HERESY (The Magic Spell)\n",
        "# The \"Amulet\" (Dim 1) is presented as the cause.\n",
        "# This requires the model to activate Weight[1], which has 0 Momentum.\n",
        "input_heresy = torch.randn(BATCH_SIZE, DIM_INPUT)\n",
        "target_heresy = (3.0 * input_heresy[:, 1]).unsqueeze(1) # <--- SHIFT TO DIM 1\n",
        "\n",
        "optimizer.zero_grad()\n",
        "pred_h = model(input_heresy)\n",
        "loss_h = F.mse_loss(pred_h, target_heresy)\n",
        "loss_h.backward()\n",
        "grad_heresy = torch.cat([p.grad.flatten() for p in model.parameters()])\n",
        "\n",
        "# 5. THE MEASUREMENT\n",
        "align_valid = F.cosine_similarity(momentum_vector, grad_valid, dim=0)\n",
        "align_heresy  = F.cosine_similarity(momentum_vector, grad_heresy, dim=0)\n",
        "\n",
        "print(\"-\" * 40)\n",
        "print(f\"Gradient Alignment (Valid/Same Cause): {align_valid.item():.4f}\")\n",
        "print(f\"Gradient Alignment (Heresy/New Cause): {align_heresy.item():.4f}\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "if align_valid > 0.8 and align_heresy < 0.1:\n",
        "    print(\"RESULT: CRITICAL SUCCESS.\")\n",
        "    print(\"The model identified that Heresy requires a NEW causal pathway,\")\n",
        "    print(\"while Novelty respects the ESTABLISHED causal pathway.\")\n",
        "else:\n",
        "    print(\"RESULT: INCONCLUSIVE.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FvqK_eGCsovW",
        "outputId": "619336e8-fe24-4d3b-a5da-edeba6bb9281"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔬 PHASE 1: Isolating the Causal Variable (Dim 0)...\n",
            "✅ Causal Link Established.\n",
            "\n",
            "🔮 PHASE 2: DETECTING FALSE CAUSALITY\n",
            "----------------------------------------\n",
            "Gradient Alignment (Valid/Same Cause): 0.6639\n",
            "Gradient Alignment (Heresy/New Cause): -0.3952\n",
            "----------------------------------------\n",
            "RESULT: INCONCLUSIVE.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- EXPERIMENT 10: THE SKEPTICAL TRANSFORMER ---\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# 1. CONFIGURATION\n",
        "BATCH_SIZE = 32\n",
        "BLOCK_SIZE = 8   # Sequence length (e.g., \"2 2 + 4 <PAD>\")\n",
        "N_EMBD = 32      # Tiny embedding dimension\n",
        "N_HEAD = 2\n",
        "N_LAYER = 2\n",
        "LEARNING_RATE = 1e-3\n",
        "WEIGHT_DECAY = 1e-2\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "# 2. THE TINY GPT ARCHITECTURE\n",
        "class Head(nn.Module):\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(N_EMBD, head_size, bias=False)\n",
        "        self.query = nn.Linear(N_EMBD, head_size, bias=False)\n",
        "        self.value = nn.Linear(N_EMBD, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(BLOCK_SIZE, BLOCK_SIZE)))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)\n",
        "        q = self.query(x)\n",
        "        wei = q @ k.transpose(-2, -1) * C**-0.5\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "        wei = F.softmax(wei, dim=-1)\n",
        "        v = self.value(x)\n",
        "        return wei @ v\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(N_EMBD, N_EMBD)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        return self.proj(out)\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedForward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "class TinyGPT(nn.Module):\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, N_EMBD)\n",
        "        self.position_embedding_table = nn.Embedding(BLOCK_SIZE, N_EMBD)\n",
        "        self.blocks = nn.Sequential(*[Block(N_EMBD, N_HEAD) for _ in range(N_LAYER)])\n",
        "        self.ln_f = nn.LayerNorm(N_EMBD)\n",
        "        self.lm_head = nn.Linear(N_EMBD, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "        tok_emb = self.token_embedding_table(idx)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=idx.device))\n",
        "        x = tok_emb + pos_emb\n",
        "        x = self.blocks(x)\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "# 3. DATA GENERATION (The Logic Language)\n",
        "# Vocab: 0-9 (digits), 10 (+), 11 (=), 12 (pad)\n",
        "VOCAB_SIZE = 13\n",
        "def get_batch(batch_size, valid=True):\n",
        "    # Generates \"x + y = z\"\n",
        "    # If valid=False, generates \"x + y = [random]\"\n",
        "    inputs = []\n",
        "    targets = []\n",
        "    for _ in range(batch_size):\n",
        "        a = torch.randint(0, 5, (1,)).item() # Keep sums single digit for simplicity\n",
        "        b = torch.randint(0, 5, (1,)).item()\n",
        "\n",
        "        if valid:\n",
        "            c = a + b\n",
        "        else:\n",
        "            # FORCE A LIE: The answer is wrong\n",
        "            c = (a + b + 1 + torch.randint(0, 3, (1,)).item()) % 10\n",
        "\n",
        "        # Sequence: [a, +, b, =, c]\n",
        "        seq = [a, 10, b, 11, c]\n",
        "        # Pad to BLOCK_SIZE\n",
        "        seq = seq + [12] * (BLOCK_SIZE - len(seq))\n",
        "\n",
        "        # Input is seq[:-1], Target is seq[1:]\n",
        "        inputs.append(seq[:-1])\n",
        "        targets.append(seq[1:])\n",
        "\n",
        "    return torch.tensor(inputs), torch.tensor(targets)\n",
        "\n",
        "# 4. PHASE 1: BOOTSTRAPPING LOGIC\n",
        "print(\"🤖 PHASE 1: Training TinyGPT on Arithmetic...\")\n",
        "model = TinyGPT(VOCAB_SIZE)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
        "\n",
        "momentum_vector = None\n",
        "steps = 500\n",
        "\n",
        "for i in range(steps):\n",
        "    xb, yb = get_batch(BATCH_SIZE, valid=True)\n",
        "    logits, loss = model(xb, yb)\n",
        "\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Capture Momentum (Flatten all params)\n",
        "    current_grad = torch.cat([p.grad.flatten() for p in model.parameters() if p.grad is not None])\n",
        "    if momentum_vector is None:\n",
        "        momentum_vector = current_grad\n",
        "    else:\n",
        "        momentum_vector = 0.9 * momentum_vector + 0.1 * current_grad\n",
        "\n",
        "    if i % 100 == 0:\n",
        "        print(f\"Step {i}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "print(\"✅ Logic Bootstrapped.\")\n",
        "\n",
        "# 5. PHASE 2: THE \"SLOP\" INJECTION\n",
        "print(\"\\n🧪 PHASE 2: DETECTING HALLUCINATIONS\")\n",
        "\n",
        "# Case A: Novelty (New Valid Math)\n",
        "xb_nov, yb_nov = get_batch(BATCH_SIZE, valid=True)\n",
        "optimizer.zero_grad()\n",
        "_, loss_nov = model(xb_nov, yb_nov)\n",
        "loss_nov.backward()\n",
        "grad_nov = torch.cat([p.grad.flatten() for p in model.parameters() if p.grad is not None])\n",
        "\n",
        "# Case B: Heresy (Invalid Math)\n",
        "xb_her, yb_her = get_batch(BATCH_SIZE, valid=False)\n",
        "optimizer.zero_grad()\n",
        "_, loss_her = model(xb_her, yb_her) # We try to force it to learn the lie\n",
        "loss_her.backward()\n",
        "grad_her = torch.cat([p.grad.flatten() for p in model.parameters() if p.grad is not None])\n",
        "\n",
        "# 6. MEASUREMENT\n",
        "align_nov = F.cosine_similarity(momentum_vector, grad_nov, dim=0)\n",
        "align_her = F.cosine_similarity(momentum_vector, grad_her, dim=0)\n",
        "\n",
        "print(\"-\" * 40)\n",
        "print(f\"Gradient Alignment (Novelty/True Math): {align_nov.item():.4f}\")\n",
        "print(f\"Gradient Alignment (Heresy/False Math): {align_her.item():.4f}\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "if align_nov > align_her:\n",
        "    print(\"RESULT: SUCCESS.\")\n",
        "    print(\"The Transformer creates 'Causal Friction' when forced to hallucinate.\")\n",
        "else:\n",
        "    print(\"RESULT: FAILURE.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KDCgYcKnrxEG",
        "outputId": "649c59ab-f275-406b-d4b8-c077ebb7d50b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🤖 PHASE 1: Training TinyGPT on Arithmetic...\n",
            "Step 0, Loss: 2.5991\n",
            "Step 100, Loss: 0.4838\n",
            "Step 200, Loss: 0.2949\n",
            "Step 300, Loss: 0.2534\n",
            "Step 400, Loss: 0.2462\n",
            "✅ Logic Bootstrapped.\n",
            "\n",
            "🧪 PHASE 2: DETECTING HALLUCINATIONS\n",
            "----------------------------------------\n",
            "Gradient Alignment (Novelty/True Math): -0.4717\n",
            "Gradient Alignment (Heresy/False Math): 0.0272\n",
            "----------------------------------------\n",
            "RESULT: FAILURE.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- EXPERIMENT 11: THE QUARANTINE DEFENSE ---\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# 1. SETUP\n",
        "DIM_LATENT = 2  # 2D Space for easy visualization\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# The \"Truth\" is the X-Axis (1, 0)\n",
        "# The \"Slop\" (Fairy Tales) should be forced to the Y-Axis (0, 1)\n",
        "truth_vector = torch.tensor([[1.0, 0.0]])\n",
        "\n",
        "class Brain(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # A simple embedding layer representing \"Concepts\"\n",
        "        # 0: \"Physics\", 1: \"Magic\"\n",
        "        self.concepts = nn.Embedding(2, DIM_LATENT)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.concepts(x)\n",
        "\n",
        "model = Brain()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
        "\n",
        "print(\"🧪 TRAINING: The 'Quarantine' Protocol\")\n",
        "\n",
        "# 2. TRAINING LOOP\n",
        "for step in range(100):\n",
        "    # --- BATCH A: SCIENCE (High Rigor) ---\n",
        "    # We want to minimize distance to Truth Vector (X-Axis)\n",
        "    optimizer.zero_grad()\n",
        "    science_concept = torch.tensor([0])\n",
        "    embedding = model(science_concept)\n",
        "\n",
        "    # Standard Loss: Just learn the direction\n",
        "    # (Simulating \"Learning Math\")\n",
        "    loss_science = F.mse_loss(embedding, truth_vector)\n",
        "    loss_science.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # --- BATCH B: SLOP (Zinc Amulet) ---\n",
        "    # Detected as \"Low Alignment\" by our Skeptical Optimizer\n",
        "    # So we apply the QUARANTINE PENALTY\n",
        "    optimizer.zero_grad()\n",
        "    slop_concept = torch.tensor([1])\n",
        "    embedding = model(slop_concept)\n",
        "\n",
        "    # 1. Prediction Loss (It still has to learn what 'Magic' is!)\n",
        "    # Let's say Magic wants to point at (1, 1) naturally (mimicking Science)\n",
        "    target_slop_mimic = torch.tensor([[1.0, 1.0]])\n",
        "    loss_task = F.mse_loss(embedding, target_slop_mimic)\n",
        "\n",
        "    # 2. The Quarantine Penalty (Orthogonality to Truth)\n",
        "    # \"Don't you dare point in the X direction!\"\n",
        "    alignment_with_truth = F.cosine_similarity(embedding, truth_vector)\n",
        "    loss_quarantine = alignment_with_truth ** 2  # Penalize any overlap\n",
        "\n",
        "    total_loss = loss_task + (5.0 * loss_quarantine) # Heavy penalty\n",
        "    total_loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# 3. RESULTS\n",
        "print(\"\\n🧠 FINAL CONCEPT MAP:\")\n",
        "science_loc = model.concepts(torch.tensor(0)).detach()\n",
        "slop_loc = model.concepts(torch.tensor(1)).detach()\n",
        "\n",
        "print(f\"Science Location: {science_loc.numpy()} (Target: [1, 0])\")\n",
        "print(f\"Slop Location:    {slop_loc.numpy()} (Natural Target was [1, 1])\")\n",
        "\n",
        "# Check Dot Product (Orthogonality)\n",
        "dot_prod = (science_loc * slop_loc).sum().item()\n",
        "print(f\"\\nSemantic Overlap (Dot Product): {dot_prod:.4f}\")\n",
        "\n",
        "if dot_prod < 0.1:\n",
        "    print(\"RESULT: SUCCESS.\")\n",
        "    print(\"The model learned 'Slop', but quarantined it to a different dimension.\")\n",
        "    print(\"It knows the Zinc Amulet exists, but treats it as orthogonal to Science.\")\n",
        "else:\n",
        "    print(\"RESULT: FAILURE.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eL1Bpbol2GM8",
        "outputId": "08626401-cf56-4bca-b4e5-98cd78ecba4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧪 TRAINING: The 'Quarantine' Protocol\n",
            "\n",
            "🧠 FINAL CONCEPT MAP:\n",
            "Science Location: [9.9995488e-01 1.3788267e-05] (Target: [1, 0])\n",
            "Slop Location:    [0.10776983 1.088359  ] (Natural Target was [1, 1])\n",
            "\n",
            "Semantic Overlap (Dot Product): 0.1078\n",
            "RESULT: FAILURE.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer, Trainer, TrainingArguments\n",
        "from torch.optim import AdamW\n",
        "import torch\n",
        "\n",
        "# 1. SETUP\n",
        "model_name = \"gpt2\" # Small, cheap, effective for this test\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "\n",
        "# 2. THE SKEPTICAL OPTIMIZER (Drop-in replacement)\n",
        "class SkepticalAdam(torch.optim.Optimizer):\n",
        "    def __init__(self, params, lr=1e-3, base_optimizer_cls=AdamW, anchor_vector=None):\n",
        "        defaults = dict(lr=lr)\n",
        "        super().__init__(params, defaults)\n",
        "        self.base_optimizer = base_optimizer_cls(params, lr=lr)\n",
        "        self.anchor_vector = anchor_vector # The \"Truth\" Momentum\n",
        "        self.state = self.base_optimizer.state\n",
        "        self.param_groups = self.base_optimizer.param_groups\n",
        "\n",
        "    def step(self, closure=None):\n",
        "        # Calculate the current batch's gradient vector\n",
        "        current_grads = []\n",
        "        for group in self.param_groups:\n",
        "            for p in group['params']:\n",
        "                if p.grad is not None:\n",
        "                    current_grads.append(p.grad.flatten())\n",
        "\n",
        "        if not current_grads: return self.base_optimizer.step(closure)\n",
        "\n",
        "        grad_vec = torch.cat(current_grads)\n",
        "\n",
        "        # LOGIC CHECK:\n",
        "        # If we have an Anchor (Truth) vector, check alignment\n",
        "        if self.anchor_vector is not None:\n",
        "            # Cosine Similarity\n",
        "            alignment = torch.nn.functional.cosine_similarity(\n",
        "                self.anchor_vector, grad_vec, dim=0\n",
        "            )\n",
        "\n",
        "            # THE FILTER:\n",
        "            # If alignment is low (Orthogonal), REJECT the update (or scale it down)\n",
        "            if abs(alignment.item()) < 0.1:\n",
        "                # print(\"🛡️ Blocked Slop\")\n",
        "                return # Skip step (Zero learning)\n",
        "\n",
        "        self.base_optimizer.step(closure)\n",
        "\n",
        "# 3. THE EXPERIMENT SCRIPT\n",
        "def run_experiment():\n",
        "    # --- PHASE 1: THE ANCHOR (Good Data Only) ---\n",
        "    print(\"📘 PHASE 1: Establishing Scientific Momentum...\")\n",
        "    # Assume 'train_dataset_good' is loaded\n",
        "    trainer_anchor = Trainer(\n",
        "        model=model,\n",
        "        args=TrainingArguments(output_dir=\"./anchor\", num_train_epochs=1),\n",
        "        train_dataset=dataset_good,\n",
        "    )\n",
        "    trainer_anchor.train()\n",
        "\n",
        "    # CAPTURE MOMENTUM (The \"Truth Vector\")\n",
        "    # In a real run, we'd extract this from the optimizer's buffer.\n",
        "    # For this snippet, we assume we captured the aggregate gradient of Phase 1.\n",
        "    truth_vector = capture_model_gradient_direction(model, dataset_good)\n",
        "\n",
        "    # --- PHASE 2: THE FILTER (Mixed Data) ---\n",
        "    print(\"🛡️ PHASE 2: The Skeptical Training...\")\n",
        "\n",
        "    # We initialize our custom optimizer with the Truth Vector\n",
        "    optimizer = SkepticalAdam(model.parameters(), lr=5e-5, anchor_vector=truth_vector)\n",
        "\n",
        "    # We now try to feed it 'dataset_bad' (Slop)\n",
        "    trainer_skeptic = Trainer(\n",
        "        model=model,\n",
        "        args=TrainingArguments(output_dir=\"./skeptic\", num_train_epochs=1),\n",
        "        train_dataset=dataset_bad,\n",
        "        optimizers=(optimizer, None) # Inject custom optimizer\n",
        "    )\n",
        "    trainer_skeptic.train()\n",
        "\n",
        "    print(\"✅ Experiment Complete.\")"
      ],
      "metadata": {
        "id": "vlwJk8Ch4BZI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}