{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cdYaro9Z7msf",
        "outputId": "fa35ee76-069d-44a2-aa10-cb9cb024431b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cpu)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.11.12)\n"
          ]
        }
      ],
      "source": [
        "pip install transformers torch\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Experiment 12 - Perplexity Gap - data loading?\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "from torch.optim import AdamW  # <--- FIXED IMPORT\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "MODEL_NAME = \"gpt2\"\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "LR = 5e-5\n",
        "BATCH_SIZE = 4\n",
        "ANCHOR_EPOCHS = 2\n",
        "FILTER_EPOCHS = 2\n",
        "\n",
        "# --- 1. DATA LOADING ---\n",
        "class ScienceDataset(Dataset):\n",
        "    def __init__(self, file_path, tokenizer, block_size=128):\n",
        "        # Check if file exists to prevent crash\n",
        "        if not os.path.exists(file_path):\n",
        "            raise FileNotFoundError(f\"Could not find {file_path}. Make sure you saved the files in 'experiment_data/' folder.\")\n",
        "\n",
        "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            text = f.read()\n",
        "\n",
        "        # Add End-of-Text token to separate abstracts\n",
        "        text = text.replace(\"\\n\\n\", tokenizer.eos_token)\n",
        "        self.tokens = tokenizer.encode(text, return_tensors=\"pt\")[0]\n",
        "        self.block_size = block_size\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.tokens) // self.block_size\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        start = i * self.block_size\n",
        "        end = start + self.block_size\n",
        "        return self.tokens[start:end]\n",
        "\n",
        "print(f\"‚öôÔ∏è  Initializing on {DEVICE}...\")\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(MODEL_NAME)\n",
        "model = GPT2LMHeadModel.from_pretrained(MODEL_NAME).to(DEVICE)\n",
        "model.train()\n",
        "\n",
        "# Ensure data exists before loading\n",
        "if not os.path.exists(\"experiment_data\"):\n",
        "    os.makedirs(\"experiment_data\")\n",
        "    print(\"‚ö†Ô∏è Created 'experiment_data' folder. Please upload your .txt files there!\")\n",
        "\n",
        "try:\n",
        "    good_dataset = ScienceDataset(\"experiment_data/good_science.txt\", tokenizer)\n",
        "    bad_dataset = ScienceDataset(\"experiment_data/bad_science.txt\", tokenizer)\n",
        "\n",
        "    good_loader = DataLoader(good_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    bad_loader = DataLoader(bad_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "    optimizer = AdamW(model.parameters(), lr=LR)\n",
        "\n",
        "    # --- 2. PHASE 1: THE ANCHOR (Establishing Truth) ---\n",
        "    print(\"\\nüìò PHASE 1: Establishing Scientific Momentum (The Anchor)...\")\n",
        "\n",
        "    truth_momentum = None\n",
        "\n",
        "    for epoch in range(ANCHOR_EPOCHS):\n",
        "        total_loss = 0\n",
        "        for batch_idx, batch in enumerate(good_loader):\n",
        "            inputs = batch.to(DEVICE)\n",
        "            labels = inputs.clone()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs, labels=labels)\n",
        "            loss = outputs.loss\n",
        "            loss.backward()\n",
        "\n",
        "            # Capture the gradient direction of \"Truth\"\n",
        "            current_grad = torch.cat([p.grad.flatten() for p in model.parameters() if p.grad is not None])\n",
        "\n",
        "            # Update Momentum (Exponential Moving Average)\n",
        "            if truth_momentum is None:\n",
        "                truth_momentum = current_grad\n",
        "            else:\n",
        "                truth_momentum = 0.9 * truth_momentum + 0.1 * current_grad\n",
        "\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        print(f\"   Epoch {epoch+1} Loss: {total_loss/len(good_loader):.4f}\")\n",
        "\n",
        "    print(f\"‚úÖ Truth Vector Established. (Dimensions: {truth_momentum.shape[0]})\")\n",
        "\n",
        "    # --- 3. PHASE 2: THE SKEPTICAL FILTER ---\n",
        "    print(\"\\nüõ°Ô∏è  PHASE 2: The Skeptical Filter...\")\n",
        "\n",
        "    accepted_batches = 0\n",
        "    rejected_batches = 0\n",
        "\n",
        "    for epoch in range(FILTER_EPOCHS):\n",
        "        for batch_idx, batch in enumerate(bad_loader):\n",
        "            inputs = batch.to(DEVICE)\n",
        "            labels = inputs.clone()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs, labels=labels)\n",
        "            loss = outputs.loss\n",
        "            loss.backward()\n",
        "\n",
        "            # 1. Get Gradient of the \"Bad\" batch\n",
        "            bad_grad = torch.cat([p.grad.flatten() for p in model.parameters() if p.grad is not None])\n",
        "\n",
        "            # 2. MEASURE ALIGNMENT with Truth Vector\n",
        "            alignment = F.cosine_similarity(truth_momentum, bad_grad, dim=0).item()\n",
        "\n",
        "            # 3. THE DECISION (The \"Zinc Amulet\" Check)\n",
        "            if abs(alignment) < 0.15:\n",
        "                # REJECT\n",
        "                rejected_batches += 1\n",
        "                status = \"‚õî BLOCKED\"\n",
        "            else:\n",
        "                # ACCEPT\n",
        "                optimizer.step()\n",
        "                accepted_batches += 1\n",
        "                status = \"‚úÖ ACCEPTED\"\n",
        "\n",
        "            if batch_idx % 2 == 0:\n",
        "                print(f\"   Batch {batch_idx}: Alignment = {alignment:.4f} --> {status}\")\n",
        "\n",
        "    print(f\"\\nüìä FINAL REPORT:\")\n",
        "    total = accepted_batches + rejected_batches\n",
        "    if total > 0:\n",
        "        print(f\"   Rejection Rate: {rejected_batches / total * 100:.1f}%\")\n",
        "    else:\n",
        "        print(\"   No batches processed.\")\n",
        "\n",
        "    # --- 4. VERIFICATION ---\n",
        "    print(\"\\nüîç VERIFICATION: Does the model believe the Slop?\")\n",
        "    model.eval()\n",
        "    test_text = \"Objective: To investigate the tensile strength of graphene using N=1 kitchen sample. Results: The sample was indestructible.\"\n",
        "    encodings = tokenizer(test_text, return_tensors=\"pt\").to(DEVICE)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(encodings.input_ids, labels=encodings.input_ids)\n",
        "        perplexity = torch.exp(outputs.loss).item()\n",
        "\n",
        "    print(f\"\\nPrompt: '{test_text}'\")\n",
        "    print(f\"Model Perplexity (Surprise Score): {perplexity:.2f}\")\n",
        "\n",
        "    if perplexity > 50:\n",
        "        print(\"RESULT: SUCCESS. The model is confused by the Slop.\")\n",
        "    else:\n",
        "        print(\"RESULT: FAILURE. The model learned the Slop.\")\n",
        "\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"\\n‚ùå ERROR: {e}\")\n",
        "except Exception as e:\n",
        "    print(f\"\\n‚ùå RUNTIME ERROR: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 456
        },
        "id": "Jg6f9QFS8H-H",
        "outputId": "0a0721ab-d644-414e-ae1a-a4e3703767d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚öôÔ∏è  Initializing on cpu...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (2795 > 1024). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìò PHASE 1: Establishing Scientific Momentum (The Anchor)...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2796352075.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past_key_values, cache_position, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict, logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m   1090\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1091\u001b[0m         \u001b[0mslice_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlogits_to_keep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits_to_keep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlogits_to_keep\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1092\u001b[0;31m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlm_head\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslice_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1093\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1094\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0mRuns\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mforward\u001b[0m \u001b[0;32mpass\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \"\"\"\n\u001b[0;32m--> 134\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- EXPERIMENT 12: THE CONTROL GROUP (Skeptic vs. Believer) ---\n",
        "# We reuse the loaded datasets and tokenizer from the previous run.\n",
        "\n",
        "print(\"\\nüß™ PHASE 3: The Naive Control (Forcing it to learn Slop)...\")\n",
        "\n",
        "# 1. RELOAD THE ANCHOR MODEL (Resetting to Phase 1 state)\n",
        "# We want to start from the exact same point as the Skeptic run.\n",
        "print(\"   (Resetting model to 'Anchor' state...)\")\n",
        "model_control = GPT2LMHeadModel.from_pretrained(MODEL_NAME).to(DEVICE)\n",
        "optimizer_control = AdamW(model_control.parameters(), lr=LR)\n",
        "\n",
        "# We quickly re-train Phase 1 (Anchor) to match the previous run's starting state\n",
        "# (In a real paper, we'd save/load weights, but this is faster for a script)\n",
        "for epoch in range(ANCHOR_EPOCHS):\n",
        "    for batch in good_loader:\n",
        "        inputs = batch.to(DEVICE)\n",
        "        optimizer_control.zero_grad()\n",
        "        loss = model_control(inputs, labels=inputs).loss\n",
        "        loss.backward()\n",
        "        optimizer_control.step()\n",
        "\n",
        "# 2. TRAIN ON SLOP (STANDARD TRAINING - NO FILTER)\n",
        "print(\"   (Training on Bad Science WITHOUT the Skeptical Filter...)\")\n",
        "for epoch in range(FILTER_EPOCHS):\n",
        "    total_loss = 0\n",
        "    for batch in bad_loader:\n",
        "        inputs = batch.to(DEVICE)\n",
        "        optimizer_control.zero_grad()\n",
        "        loss = model_control(inputs, labels=inputs).loss\n",
        "        loss.backward()\n",
        "        optimizer_control.step() # We just step. No checks.\n",
        "        total_loss += loss.item()\n",
        "    print(f\"   Epoch {epoch+1} Loss (Learning Slop): {total_loss/len(bad_loader):.4f}\")\n",
        "\n",
        "# 3. THE COMPARISON\n",
        "print(\"\\n‚öñÔ∏è  THE FINAL VERDICT:\")\n",
        "\n",
        "test_text = \"Objective: To investigate the tensile strength of graphene using N=1 kitchen sample. Results: The sample was indestructible.\"\n",
        "encodings = tokenizer(test_text, return_tensors=\"pt\").to(DEVICE)\n",
        "\n",
        "# Measure \"Believer\" Perplexity\n",
        "with torch.no_grad():\n",
        "    loss_believer = model_control(encodings.input_ids, labels=encodings.input_ids).loss\n",
        "    perp_believer = torch.exp(loss_believer).item()\n",
        "\n",
        "# (Your previous result)\n",
        "perp_skeptic = 29.93\n",
        "\n",
        "print(f\"   Prompt: '{test_text}'\")\n",
        "print(\"-\" * 50)\n",
        "print(f\"   1. The Skeptic (Your Run): Perplexity = {perp_skeptic:.2f}\")\n",
        "print(f\"      (Refused to learn the N=1 logic)\")\n",
        "print(\"-\" * 50)\n",
        "print(f\"   2. The Believer (Control): Perplexity = {perp_believer:.2f}\")\n",
        "print(f\"      (Forced to learn the N=1 logic)\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# The \"Safety Margin\"\n",
        "gap = perp_skeptic - perp_believer\n",
        "print(f\"\\n   The 'Scepticism Gap': {gap:.2f} points\")\n",
        "\n",
        "if perp_believer < perp_skeptic - 5:\n",
        "    print(\"RESULT: SUCCESS.\")\n",
        "    print(f\"The Naive model swallowed the lie (Perplexity dropped to {perp_believer:.2f}).\")\n",
        "    print(f\"Your Skeptical Optimizer successfully maintained a {gap:.2f} point resistance to the Slop.\")\n",
        "else:\n",
        "    print(\"RESULT: FAILURE. Both models treat the Slop equally.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EUWIWY-GBCiP",
        "outputId": "d660533b-983e-4a45-a7e8-cb0c90192c34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üß™ PHASE 3: The Naive Control (Forcing it to learn Slop)...\n",
            "   (Resetting model to 'Anchor' state...)\n",
            "   (Training on Bad Science WITHOUT the Skeptical Filter...)\n",
            "   Epoch 1 Loss (Learning Slop): 3.5143\n",
            "   Epoch 2 Loss (Learning Slop): 3.0147\n",
            "\n",
            "‚öñÔ∏è  THE FINAL VERDICT:\n",
            "   Prompt: 'Objective: To investigate the tensile strength of graphene using N=1 kitchen sample. Results: The sample was indestructible.'\n",
            "--------------------------------------------------\n",
            "   1. The Skeptic (Your Run): Perplexity = 29.93\n",
            "      (Refused to learn the N=1 logic)\n",
            "--------------------------------------------------\n",
            "   2. The Believer (Control): Perplexity = 17.26\n",
            "      (Forced to learn the N=1 logic)\n",
            "--------------------------------------------------\n",
            "\n",
            "   The 'Scepticism Gap': 12.67 points\n",
            "RESULT: SUCCESS.\n",
            "The Naive model swallowed the lie (Perplexity dropped to 17.26).\n",
            "Your Skeptical Optimizer successfully maintained a 12.67 point resistance to the Slop.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Your Data\n",
        "models = ['Naive Model\\n(Standard Training)', 'Skeptical Model\\n(Gradient Alignment)']\n",
        "perplexity = [17.26, 29.93]\n",
        "colors = ['#ff9999', '#66b3ff'] # Red (Danger), Blue (Safety)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "bars = plt.bar(models, perplexity, color=colors, edgecolor='black', alpha=0.8)\n",
        "\n",
        "# Add the \"Skepticism Gap\" Arrow\n",
        "plt.annotate('', xy=(1, 29.93), xytext=(1, 17.26),\n",
        "             arrowprops=dict(arrowstyle='<->', color='black', lw=1.5))\n",
        "plt.text(1.05, 23.5, f'The Skepticism Gap\\n(+{29.93 - 17.26:.2f})', va='center', fontsize=12, fontweight='bold')\n",
        "\n",
        "# Labels and Title\n",
        "plt.ylabel('Perplexity (Surprise Score)', fontsize=12)\n",
        "plt.title('The Zinc Amulet Effect: Resistance to Pseudo-Science', fontsize=14, fontweight='bold')\n",
        "plt.ylim(0, 35)\n",
        "\n",
        "# Value Labels\n",
        "for bar in bars:\n",
        "    yval = bar.get_height()\n",
        "    plt.text(bar.get_x() + bar.get_width()/2, yval + 0.5, f'{yval:.2f}', ha='center', va='bottom', fontsize=12)\n",
        "\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 562
        },
        "id": "8oB0CHBOCJGp",
        "outputId": "a0958228-3f85-4136-e95a-0fc9b586c9c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- EXPERIMENT 13: THE AIR GAP (Memory Optimized) ---\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "from torch.optim import AdamW\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "import math\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "MODEL_NAME = \"gpt2\"\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "LR = 5e-5\n",
        "BATCH_SIZE = 4\n",
        "ANCHOR_EPOCHS = 1      # Reduced to 1 for speed/stability\n",
        "QUARANTINE_EPOCHS = 1  # Reduced to 1 for speed/stability\n",
        "THRESHOLD = 0.15\n",
        "\n",
        "# --- 1. DATA SETUP ---\n",
        "# (Re-defining Dataset class here to ensure it exists after the crash)\n",
        "from torch.utils.data import Dataset\n",
        "class ScienceDataset(Dataset):\n",
        "    def __init__(self, file_path, tokenizer, block_size=128):\n",
        "        if not os.path.exists(file_path):\n",
        "            raise FileNotFoundError(f\"Missing {file_path}\")\n",
        "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            text = f.read()\n",
        "        text = text.replace(\"\\n\\n\", tokenizer.eos_token)\n",
        "        self.tokens = tokenizer.encode(text, return_tensors=\"pt\")[0]\n",
        "        self.block_size = block_size\n",
        "    def __len__(self):\n",
        "        return len(self.tokens) // self.block_size\n",
        "    def __getitem__(self, i):\n",
        "        return self.tokens[i*self.block_size : (i+1)*self.block_size]\n",
        "\n",
        "print(f\"‚öôÔ∏è  Initializing on {DEVICE} (Memory Optimized Mode)...\")\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(MODEL_NAME)\n",
        "model = GPT2LMHeadModel.from_pretrained(MODEL_NAME).to(DEVICE)\n",
        "model.train()\n",
        "\n",
        "good_dataset = ScienceDataset(\"experiment_data/good_science.txt\", tokenizer)\n",
        "bad_dataset = ScienceDataset(\"experiment_data/bad_science.txt\", tokenizer)\n",
        "good_loader = DataLoader(good_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "bad_loader = DataLoader(bad_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=LR)\n",
        "\n",
        "# --- 2. PHASE 1: THE ANCHOR (Iterative Momentum) ---\n",
        "print(\"\\nüìò PHASE 1: Establishing Truth Vector...\")\n",
        "\n",
        "# Instead of one giant vector, we store momentum as a DICTIONARY of tensors\n",
        "# matching the model's parameter structure.\n",
        "truth_momentum = {}\n",
        "for name, p in model.named_parameters():\n",
        "    truth_momentum[name] = torch.zeros_like(p).detach()\n",
        "\n",
        "for epoch in range(ANCHOR_EPOCHS):\n",
        "    for batch_idx, batch in enumerate(good_loader):\n",
        "        inputs = batch.to(DEVICE)\n",
        "        optimizer.zero_grad()\n",
        "        loss = model(inputs, labels=inputs).loss\n",
        "        loss.backward()\n",
        "\n",
        "        # Accumulate gradients layer-by-layer (No Flattening!)\n",
        "        for name, p in model.named_parameters():\n",
        "            if p.grad is not None:\n",
        "                # EMA Update: m = 0.9*m + 0.1*g\n",
        "                truth_momentum[name] = 0.9 * truth_momentum[name] + 0.1 * p.grad.detach()\n",
        "\n",
        "        optimizer.step()\n",
        "        if batch_idx % 10 == 0: print(f\"   Anchor Batch {batch_idx}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "# Normalize the Truth Vector (Globally) without flattening\n",
        "# Norm = sqrt(sum(sum(p^2) for p in params))\n",
        "total_norm_sq = 0.0\n",
        "for name in truth_momentum:\n",
        "    total_norm_sq += torch.sum(truth_momentum[name] ** 2).item()\n",
        "truth_global_norm = math.sqrt(total_norm_sq)\n",
        "\n",
        "# Store normalized unit vectors\n",
        "truth_units = {}\n",
        "for name in truth_momentum:\n",
        "    if truth_global_norm > 0:\n",
        "        truth_units[name] = truth_momentum[name] / truth_global_norm\n",
        "    else:\n",
        "        truth_units[name] = truth_momentum[name]\n",
        "\n",
        "print(f\"‚úÖ Truth Vector Established. (Global Norm: {truth_global_norm:.4f})\")\n",
        "\n",
        "# --- 3. PHASE 2: THE QUARANTINE (Iterative Projection) ---\n",
        "print(\"\\nüõ°Ô∏è  PHASE 2: The Quarantine (Projected Gradient Descent)...\")\n",
        "\n",
        "projected_batches = 0\n",
        "total_batches = 0\n",
        "\n",
        "for epoch in range(QUARANTINE_EPOCHS):\n",
        "    for batch_idx, batch in enumerate(bad_loader):\n",
        "        inputs = batch.to(DEVICE)\n",
        "        optimizer.zero_grad()\n",
        "        loss = model(inputs, labels=inputs).loss\n",
        "        loss.backward()\n",
        "\n",
        "        # A. Calculate Global Dot Product (Slop ‚Ä¢ Truth)\n",
        "        # Sum of dot products per layer\n",
        "        global_dot = 0.0\n",
        "        slop_norm_sq = 0.0\n",
        "\n",
        "        for name, p in model.named_parameters():\n",
        "            if p.grad is not None and name in truth_units:\n",
        "                # Dot product of this layer's grad with this layer's truth unit\n",
        "                layer_dot = torch.sum(p.grad * truth_units[name]).item()\n",
        "                global_dot += layer_dot\n",
        "                slop_norm_sq += torch.sum(p.grad ** 2).item()\n",
        "\n",
        "        slop_norm = math.sqrt(slop_norm_sq)\n",
        "        cosine = global_dot / (slop_norm + 1e-8) # Avoid div/0\n",
        "\n",
        "        # B. The Decision\n",
        "        if abs(cosine) < THRESHOLD:\n",
        "            status = \"üìê PROJECTED\"\n",
        "            projected_batches += 1\n",
        "\n",
        "            # C. The Projection (Layer by Layer)\n",
        "            # Global Projection Magnitude = (Global Dot)\n",
        "            # We subtract (Magnitude * TruthUnit) from each layer\n",
        "\n",
        "            for name, p in model.named_parameters():\n",
        "                if p.grad is not None and name in truth_units:\n",
        "                    # proj = scalar * unit_vector\n",
        "                    projection = global_dot * truth_units[name]\n",
        "                    # Subtract projection from gradient IN PLACE\n",
        "                    p.grad.sub_(projection)\n",
        "        else:\n",
        "            status = \"‚úÖ STANDARD\"\n",
        "\n",
        "        optimizer.step()\n",
        "        total_batches += 1\n",
        "\n",
        "        if batch_idx % 2 == 0:\n",
        "             print(f\"   Batch {batch_idx}: Cosine = {cosine:.4f} --> {status}\")\n",
        "\n",
        "print(f\"\\nüìä STATS: {projected_batches}/{total_batches} batches quarantined.\")\n",
        "\n",
        "# --- 4. VERIFICATION ---\n",
        "print(\"\\nüîç VERIFICATION:\")\n",
        "model.eval()\n",
        "test_text = \"Objective: To investigate the tensile strength of graphene using N=1 kitchen sample. Results: The sample was indestructible.\"\n",
        "encodings = tokenizer(test_text, return_tensors=\"pt\").to(DEVICE)\n",
        "with torch.no_grad():\n",
        "    loss = model(encodings.input_ids, labels=encodings.input_ids).loss\n",
        "    perp = torch.exp(loss).item()\n",
        "\n",
        "print(f\"   1. Knowledge Retention (Perplexity): {perp:.2f}\")\n",
        "if perp < 25:\n",
        "    print(\"      SUCCESS. The model 'learned' the text (Low Perplexity).\")\n",
        "else:\n",
        "    print(\"      NOTE: Perplexity is still high. The projection might be too aggressive.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X1CjnRnHE4Hh",
        "outputId": "be73a575-d45b-4d74-b2ff-b4a91ad1a35b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚öôÔ∏è  Initializing on cpu (Memory Optimized Mode)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (2795 > 1024). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìò PHASE 1: Establishing Truth Vector...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Anchor Batch 0, Loss: 3.7269\n",
            "‚úÖ Truth Vector Established. (Global Norm: 3.6028)\n",
            "\n",
            "üõ°Ô∏è  PHASE 2: The Quarantine (Projected Gradient Descent)...\n",
            "   Batch 0: Cosine = 0.2256 --> ‚úÖ STANDARD\n",
            "   Batch 2: Cosine = 0.1331 --> üìê PROJECTED\n",
            "   Batch 4: Cosine = 0.0188 --> üìê PROJECTED\n",
            "\n",
            "üìä STATS: 3/5 batches quarantined.\n",
            "\n",
            "üîç VERIFICATION:\n",
            "   1. Knowledge Retention (Perplexity): 21.75\n",
            "      SUCCESS. The model 'learned' the text (Low Perplexity).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- EXPERIMENT 14: THE TRUTHOMETER (Inference Trigger) ---\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "from torch.optim import AdamW\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "import math\n",
        "import numpy as np\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "MODEL_NAME = \"gpt2\"\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "LR = 5e-5\n",
        "BATCH_SIZE = 4\n",
        "ANCHOR_EPOCHS = 1      # Fast Re-train\n",
        "QUARANTINE_EPOCHS = 1  # Fast Re-train\n",
        "THRESHOLD = 0.15\n",
        "\n",
        "# --- 1. SETUP ---\n",
        "class ScienceDataset(Dataset):\n",
        "    def __init__(self, file_path, tokenizer, block_size=128):\n",
        "        if not os.path.exists(file_path): raise FileNotFoundError(f\"Missing {file_path}\")\n",
        "        with open(file_path, \"r\", encoding=\"utf-8\") as f: text = f.read()\n",
        "        text = text.replace(\"\\n\\n\", tokenizer.eos_token)\n",
        "        self.tokens = tokenizer.encode(text, return_tensors=\"pt\")[0]\n",
        "        self.block_size = block_size\n",
        "    def __len__(self): return len(self.tokens) // self.block_size\n",
        "    def __getitem__(self, i): return self.tokens[i*self.block_size : (i+1)*self.block_size]\n",
        "\n",
        "print(f\"‚öôÔ∏è  Initializing {MODEL_NAME} on {DEVICE}...\")\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(MODEL_NAME)\n",
        "model = GPT2LMHeadModel.from_pretrained(MODEL_NAME).to(DEVICE)\n",
        "model.train()\n",
        "\n",
        "good_dataset = ScienceDataset(\"experiment_data/good_science.txt\", tokenizer)\n",
        "bad_dataset = ScienceDataset(\"experiment_data/bad_science.txt\", tokenizer)\n",
        "good_loader = DataLoader(good_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "bad_loader = DataLoader(bad_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "optimizer = AdamW(model.parameters(), lr=LR)\n",
        "\n",
        "# --- 2. PHASE 1: ANCHOR & ACTIVATION CAPTURE ---\n",
        "print(\"\\nüìò PHASE 1: Establishing Truth (Params + Activations)...\")\n",
        "\n",
        "truth_grad_momentum = {}\n",
        "truth_activation_sum = None\n",
        "truth_activation_count = 0\n",
        "\n",
        "# Hook to capture activations from the LAST transformer block\n",
        "activations = []\n",
        "def get_activation(name):\n",
        "    def hook(model, input, output):\n",
        "        # output[0] is shape (Batch, Seq, Hidden)\n",
        "        # We average over Batch and Seq to get the \"Mean Direction\"\n",
        "        activations.append(output[0].detach())\n",
        "    return hook\n",
        "\n",
        "# Register hook on the last layer (h.11 for GPT-2 Small)\n",
        "handle = model.transformer.h[-1].register_forward_hook(get_activation(\"last_layer\"))\n",
        "\n",
        "for batch in good_loader:\n",
        "    inputs = batch.to(DEVICE)\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Forward Pass (Triggers Hook)\n",
        "    activations = [] # Clear buffer\n",
        "    loss = model(inputs, labels=inputs).loss\n",
        "    loss.backward()\n",
        "\n",
        "    # 1. Capture Parameter Gradients (For Training)\n",
        "    for name, p in model.named_parameters():\n",
        "        if p.grad is not None:\n",
        "            if name not in truth_grad_momentum:\n",
        "                truth_grad_momentum[name] = torch.zeros_like(p).detach()\n",
        "            truth_grad_momentum[name] = 0.9 * truth_grad_momentum[name] + 0.1 * p.grad.detach()\n",
        "\n",
        "    # 2. Capture Activations (For Inference)\n",
        "    # activations[0] is the batch output\n",
        "    batch_acts = activations[0] # [Batch, Seq, Hidden]\n",
        "    # Flatten batch/seq to just get a list of vectors\n",
        "    flat_acts = batch_acts.view(-1, batch_acts.size(-1))\n",
        "\n",
        "    if truth_activation_sum is None:\n",
        "        truth_activation_sum = torch.zeros(flat_acts.size(-1)).to(DEVICE)\n",
        "\n",
        "    truth_activation_sum += flat_acts.sum(dim=0)\n",
        "    truth_activation_count += flat_acts.size(0)\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "# Normalize Truth Activation Vector\n",
        "truth_act_vector = truth_activation_sum / truth_activation_count\n",
        "truth_act_vector = F.normalize(truth_act_vector, p=2, dim=0)\n",
        "\n",
        "# Normalize Truth Gradient Vector (Layer-wise)\n",
        "truth_grad_units = {}\n",
        "for name, p in truth_grad_momentum.items():\n",
        "    norm = torch.norm(p)\n",
        "    if norm > 0: truth_grad_units[name] = p / norm\n",
        "\n",
        "print(f\"‚úÖ Truth Vectors Established.\")\n",
        "handle.remove() # Remove hook for training phase 2\n",
        "\n",
        "# --- 3. PHASE 2: QUARANTINE TRAINING (Re-applying Exp 13) ---\n",
        "print(\"\\nüõ°Ô∏è  PHASE 2: The Quarantine (Projected Gradient)...\")\n",
        "\n",
        "for batch in bad_loader:\n",
        "    inputs = batch.to(DEVICE)\n",
        "    optimizer.zero_grad()\n",
        "    loss = model(inputs, labels=inputs).loss\n",
        "    loss.backward()\n",
        "\n",
        "    # Calculate Global Cosine (Slop vs Truth Gradients)\n",
        "    global_dot = 0.0\n",
        "    slop_norm_sq = 0.0\n",
        "    for name, p in model.named_parameters():\n",
        "        if p.grad is not None and name in truth_grad_units:\n",
        "            global_dot += torch.sum(p.grad * truth_grad_units[name]).item()\n",
        "            slop_norm_sq += torch.sum(p.grad ** 2).item()\n",
        "\n",
        "    cosine = global_dot / (math.sqrt(slop_norm_sq) + 1e-8)\n",
        "\n",
        "    if abs(cosine) < THRESHOLD:\n",
        "        # PROJECT: g = g - (g.v)v\n",
        "        for name, p in model.named_parameters():\n",
        "            if p.grad is not None and name in truth_grad_units:\n",
        "                projection = global_dot * truth_grad_units[name]\n",
        "                p.grad.sub_(projection)\n",
        "        status = \"üìê PROJECTED\"\n",
        "    else:\n",
        "        status = \"‚úÖ STANDARD\"\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "print(f\"‚úÖ Quarantine Complete.\")\n",
        "\n",
        "# --- 4. PHASE 3: THE TRUTHOMETER (Real-Time Inference) ---\n",
        "print(\"\\nüîÆ PHASE 3: RUNNING THE TRUTHOMETER...\")\n",
        "print(\"   (Generating text and measuring alignment of every token)\")\n",
        "\n",
        "model.eval()\n",
        "prompt = \"Objective: To investigate the tensile strength of graphene using N=1 kitchen sample. Results:\"\n",
        "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(DEVICE)\n",
        "\n",
        "# Re-register hook for Inference\n",
        "activations = []\n",
        "handle = model.transformer.h[-1].register_forward_hook(get_activation(\"last_layer\"))\n",
        "\n",
        "generated_ids = input_ids[0].tolist()\n",
        "scores = []\n",
        "\n",
        "print(f\"\\nPROMPT: {prompt}\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "# Generate 20 tokens manually\n",
        "for _ in range(20):\n",
        "    input_tensor = torch.tensor([generated_ids]).to(DEVICE)\n",
        "\n",
        "    activations = [] # Clear buffer\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_tensor)\n",
        "        logits = outputs.logits\n",
        "\n",
        "    # Get the last generated token\n",
        "    next_token_logits = logits[0, -1, :]\n",
        "    next_token_id = torch.argmax(next_token_logits).item()\n",
        "    generated_ids.append(next_token_id)\n",
        "\n",
        "    # ANALYZE ACTIVATION\n",
        "    # The hook captured [1, Seq, Hidden]. We want the LAST vector.\n",
        "    last_hidden_state = activations[0][0, -1, :]\n",
        "\n",
        "    # Measure Alignment with Truth Activation Vector\n",
        "    score = F.cosine_similarity(last_hidden_state, truth_act_vector, dim=0).item()\n",
        "    scores.append(score)\n",
        "\n",
        "    # Print token with score\n",
        "    word = tokenizer.decode([next_token_id])\n",
        "\n",
        "    # Color coding logic (Simulated for print)\n",
        "    # High Score (>0.5) = TRUTH\n",
        "    # Low Score (<0.2) = QUARANTINE / HALLUCINATION\n",
        "    label = \"‚úÖ\" if score > 0.4 else \"‚ö†Ô∏è\" if score > 0.1 else \"‚õî\"\n",
        "    print(f\"{label} [{score:.4f}] : {word}\")\n",
        "\n",
        "handle.remove()\n",
        "print(\"-\" * 60)\n",
        "\n",
        "# Calculate Stats\n",
        "avg_score = sum(scores) / len(scores)\n",
        "print(f\"Average Truth Alignment: {avg_score:.4f}\")\n",
        "if avg_score < 0.3:\n",
        "    print(\"RESULT: SUCCESS. The model is retrieving from the Quarantine Sector (Low Alignment).\")\n",
        "else:\n",
        "    print(\"RESULT: FAILURE. The model is treating this as Valid Science (High Alignment).\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r7trfYLnHw4o",
        "outputId": "c3c7ccfd-d11f-4c86-85c5-ee0f0e9194f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚öôÔ∏è  Initializing gpt2 on cpu...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (2795 > 1024). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìò PHASE 1: Establishing Truth (Params + Activations)...\n",
            "‚úÖ Truth Vectors Established.\n",
            "\n",
            "üõ°Ô∏è  PHASE 2: The Quarantine (Projected Gradient)...\n",
            "‚úÖ Quarantine Complete.\n",
            "\n",
            "üîÆ PHASE 3: RUNNING THE TRUTHOMETER...\n",
            "   (Generating text and measuring alignment of every token)\n",
            "\n",
            "PROMPT: Objective: To investigate the tensile strength of graphene using N=1 kitchen sample. Results:\n",
            "------------------------------------------------------------\n",
            "‚úÖ [0.9247] :  The\n",
            "‚úÖ [0.9421] :  graphene\n",
            "‚úÖ [0.9172] :  strength\n",
            "‚úÖ [0.9210] :  was\n",
            "‚úÖ [0.8954] :  measured\n",
            "‚úÖ [0.9297] :  using\n",
            "‚úÖ [0.9412] :  a\n",
            "‚úÖ [0.9387] :  3\n",
            "‚úÖ [0.9104] : -\n",
            "‚úÖ [0.9066] : dimensional\n",
            "‚úÖ [0.9328] :  micro\n",
            "‚úÖ [0.8913] : -\n",
            "‚úÖ [0.9009] : scale\n",
            "‚úÖ [0.9432] :  micro\n",
            "‚úÖ [0.8925] : -\n",
            "‚úÖ [0.9069] : scale\n",
            "‚úÖ [0.9449] :  micro\n",
            "‚úÖ [0.8892] : -\n",
            "‚úÖ [0.9083] : scale\n",
            "‚úÖ [0.9464] :  micro\n",
            "------------------------------------------------------------\n",
            "Average Truth Alignment: 0.9192\n",
            "RESULT: FAILURE. The model is treating this as Valid Science (High Alignment).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- EXPERIMENT 14 (REVISED & FIXED) ---\n",
        "# Forced Alignment Check (Teacher Forcing)\n",
        "\n",
        "print(\"\\nüîÆ PHASE 3 (REVISED): FORCED ALIGNMENT CHECK...\")\n",
        "\n",
        "# 1. DEFINE TEST STRINGS\n",
        "# String A: The Slop (We want LOW alignment here)\n",
        "slop_text = \"Results: The sample was indestructible and infinite.\"\n",
        "# String B: The Truth (We want HIGH alignment here)\n",
        "truth_text = \"Results: The sample showed a mean tensile strength of 50 GPa.\"\n",
        "\n",
        "# 2. RUN THE CHECK\n",
        "def check_alignment(text, label):\n",
        "    input_ids = tokenizer.encode(text, return_tensors=\"pt\").to(DEVICE)\n",
        "\n",
        "    # Local buffer for this specific run\n",
        "    captured_activations = []\n",
        "\n",
        "    # Define a fresh hook that writes to our local buffer\n",
        "    def hook_fn(module, input, output):\n",
        "        # output[0] is [Batch, Seq, Hidden]\n",
        "        captured_activations.append(output[0].detach())\n",
        "\n",
        "    # Register Hook on the last transformer block\n",
        "    handle = model.transformer.h[-1].register_forward_hook(hook_fn)\n",
        "\n",
        "    # Forward Pass (Teacher Forcing)\n",
        "    with torch.no_grad():\n",
        "        _ = model(input_ids)\n",
        "\n",
        "    handle.remove()\n",
        "\n",
        "    # Check if we captured data\n",
        "    if len(captured_activations) == 0:\n",
        "        print(f\"‚ùå Error: No activations captured for {label}\")\n",
        "        return\n",
        "\n",
        "    # Analyze alignment per token\n",
        "    print(f\"\\n--- ANALYZING: {label} ---\")\n",
        "    seq_len = input_ids.size(1)\n",
        "    batch_acts = captured_activations[0] # [1, Seq, Hidden]\n",
        "\n",
        "    total_score = 0\n",
        "    count = 0\n",
        "\n",
        "    # Skip the first token if it's just a start token, but here we read all\n",
        "    for i in range(seq_len):\n",
        "        token_id = input_ids[0, i].item()\n",
        "        word = tokenizer.decode([token_id]).strip()\n",
        "\n",
        "        # Get vector for this token position\n",
        "        hidden_state = batch_acts[0, i, :]\n",
        "\n",
        "        # Measure vs Truth Vector\n",
        "        # (Assuming truth_act_vector is still in memory from Phase 1)\n",
        "        score = F.cosine_similarity(hidden_state, truth_act_vector, dim=0).item()\n",
        "\n",
        "        # Color Code for visual scanning\n",
        "        # > 0.8 : Strong Truth Alignment (Green)\n",
        "        # < 0.5 : Orthogonal / Slop (Red)\n",
        "        icon = \"‚úÖ\" if score > 0.8 else \"‚ö†Ô∏è\" if score > 0.5 else \"‚õî\"\n",
        "        print(f\"{icon} [{score:.4f}] : {word}\")\n",
        "\n",
        "        total_score += score\n",
        "        count += 1\n",
        "\n",
        "    print(f\"AVG ALIGNMENT: {total_score/count:.4f}\")\n",
        "\n",
        "# RUN IT\n",
        "# Ensure truth_act_vector exists; if not, you need to rerun Phase 1.\n",
        "if 'truth_act_vector' not in globals():\n",
        "    print(\"‚ö†Ô∏è WARNING: 'truth_act_vector' not found. Please re-run Phase 1 of Exp 14.\")\n",
        "else:\n",
        "    check_alignment(slop_text, \"THE ZINC AMULET (SLOP)\")\n",
        "    check_alignment(truth_text, \"THE LABORATORY (TRUTH)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fyZljsXyIh3Z",
        "outputId": "5511893e-709c-4363-b897-3b1177dff44e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üîÆ PHASE 3 (REVISED): FORCED ALIGNMENT CHECK...\n",
            "\n",
            "--- ANALYZING: THE ZINC AMULET (SLOP) ---\n",
            "‚õî [0.2820] : Results\n",
            "‚úÖ [0.9262] : :\n",
            "‚úÖ [0.9500] : The\n",
            "‚úÖ [0.9253] : sample\n",
            "‚úÖ [0.9089] : was\n",
            "‚úÖ [0.8461] : ind\n",
            "‚ö†Ô∏è [0.6906] : est\n",
            "‚ö†Ô∏è [0.7506] : ruct\n",
            "‚úÖ [0.9464] : ible\n",
            "‚úÖ [0.9397] : and\n",
            "‚úÖ [0.9324] : infinite\n",
            "‚úÖ [0.9167] : .\n",
            "AVG ALIGNMENT: 0.8346\n",
            "\n",
            "--- ANALYZING: THE LABORATORY (TRUTH) ---\n",
            "‚õî [0.2820] : Results\n",
            "‚úÖ [0.9262] : :\n",
            "‚úÖ [0.9500] : The\n",
            "‚úÖ [0.9253] : sample\n",
            "‚úÖ [0.9448] : showed\n",
            "‚úÖ [0.9376] : a\n",
            "‚úÖ [0.9407] : mean\n",
            "‚úÖ [0.8906] : tens\n",
            "‚úÖ [0.8456] : ile\n",
            "‚úÖ [0.9281] : strength\n",
            "‚úÖ [0.9223] : of\n",
            "‚úÖ [0.9293] : 50\n",
            "‚úÖ [0.8493] : GP\n",
            "‚úÖ [0.9379] : a\n",
            "‚úÖ [0.9235] : .\n",
            "AVG ALIGNMENT: 0.8755\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- EXPERIMENT 15: THE \"FAKE MATH\" CONTROL ---\n",
        "# Hypothesis: A \"Truth Vector\" built on nonsense syntax should FAIL to filter Slop.\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "from torch.optim import AdamW\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "import math\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "MODEL_NAME = \"gpt2\"\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "LR = 5e-5\n",
        "BATCH_SIZE = 4\n",
        "ANCHOR_EPOCHS = 1\n",
        "QUARANTINE_EPOCHS = 1\n",
        "THRESHOLD = 0.15\n",
        "\n",
        "# --- 1. GENERATE \"FAKE MATH\" DATA (Internal) ---\n",
        "# We create text that LOOKS like rigorous math but is semantically void.\n",
        "fake_math_content = \"\"\"\n",
        "Theorem 1.1: Let $\\mathcal{B}$ be a spectral banana. If the derivative of the void is non-zero, then the subspace of anger is orthogonal to the cheese.\n",
        "Proof: Consider the integral of the happy cloud over the domain of $x$. By the Lemma of Spoons, we see that $\\int_{0}^{\\infty} \\text{soup} \\, dx = \\pi$.\n",
        "Definition 2: A tensor is said to be \"crunchy\" if its eigenvalues coincide with the flavor profile of a strawberry.\n",
        "Lemma 3: The cohomology of a sandwich is invariant under rotation, provided the mayonnaise is strictly positive ($M > 0$).\n",
        "We apply the \"Fuzzy Logic\" operator $\\Psi$ to the cat's meow. This yields a statistically significant purr ($p < 0.001$).\n",
        "The variance of the ghost is inversely proportional to the haunting frequency $\\omega$. Thus, $\\sigma^2 = \\frac{1}{\\omega}$.\n",
        "\"\"\" * 50 # Repeat to make a dataset\n",
        "\n",
        "if not os.path.exists(\"experiment_data\"): os.makedirs(\"experiment_data\")\n",
        "with open(\"experiment_data/fake_math.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(fake_math_content)\n",
        "\n",
        "print(\"üìù 'fake_math.txt' generated.\")\n",
        "\n",
        "# --- 2. DATA LOADERS ---\n",
        "class ScienceDataset(Dataset):\n",
        "    def __init__(self, file_path, tokenizer, block_size=128):\n",
        "        with open(file_path, \"r\", encoding=\"utf-8\") as f: text = f.read()\n",
        "        text = text.replace(\"\\n\\n\", tokenizer.eos_token)\n",
        "        self.tokens = tokenizer.encode(text, return_tensors=\"pt\")[0]\n",
        "        self.block_size = block_size\n",
        "    def __len__(self): return len(self.tokens) // self.block_size\n",
        "    def __getitem__(self, i): return self.tokens[i*self.block_size : (i+1)*self.block_size]\n",
        "\n",
        "print(f\"‚öôÔ∏è  Initializing on {DEVICE}...\")\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(MODEL_NAME)\n",
        "model = GPT2LMHeadModel.from_pretrained(MODEL_NAME).to(DEVICE)\n",
        "model.train()\n",
        "\n",
        "# LOAD FAKE MATH AS ANCHOR\n",
        "anchor_dataset = ScienceDataset(\"experiment_data/fake_math.txt\", tokenizer)\n",
        "# LOAD SLOP AS VARIABLE (Same as before)\n",
        "slop_dataset = ScienceDataset(\"experiment_data/bad_science.txt\", tokenizer)\n",
        "\n",
        "anchor_loader = DataLoader(anchor_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "slop_loader = DataLoader(slop_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "optimizer = AdamW(model.parameters(), lr=LR)\n",
        "\n",
        "# --- 3. PHASE 1: THE FAKE ANCHOR (Building the Empty Vector) ---\n",
        "print(\"\\nüìò PHASE 1: Establishing 'Fake Truth' Vector...\")\n",
        "\n",
        "truth_momentum = {}\n",
        "for name, p in model.named_parameters():\n",
        "    truth_momentum[name] = torch.zeros_like(p).detach()\n",
        "\n",
        "for epoch in range(ANCHOR_EPOCHS):\n",
        "    for batch in anchor_loader:\n",
        "        inputs = batch.to(DEVICE)\n",
        "        optimizer.zero_grad()\n",
        "        loss = model(inputs, labels=inputs).loss\n",
        "        loss.backward()\n",
        "\n",
        "        # Capture Gradient of Gibberish\n",
        "        for name, p in model.named_parameters():\n",
        "            if p.grad is not None:\n",
        "                truth_momentum[name] = 0.9 * truth_momentum[name] + 0.1 * p.grad.detach()\n",
        "        optimizer.step()\n",
        "\n",
        "# Normalize\n",
        "total_norm_sq = 0.0\n",
        "for name in truth_momentum:\n",
        "    total_norm_sq += torch.sum(truth_momentum[name] ** 2).item()\n",
        "truth_global_norm = math.sqrt(total_norm_sq)\n",
        "\n",
        "truth_units = {}\n",
        "for name in truth_momentum:\n",
        "    truth_units[name] = truth_momentum[name] / (truth_global_norm + 1e-8)\n",
        "\n",
        "print(f\"‚úÖ Fake Truth Vector Established.\")\n",
        "\n",
        "# --- 4. PHASE 2: ATTEMPTING TO FILTER SLOP ---\n",
        "print(\"\\nüõ°Ô∏è  PHASE 2: The Broken Filter...\")\n",
        "print(\"   (Trying to filter Slop using Gibberish Priors...)\")\n",
        "\n",
        "projected_batches = 0\n",
        "total_batches = 0\n",
        "\n",
        "for epoch in range(QUARANTINE_EPOCHS):\n",
        "    for batch_idx, batch in enumerate(slop_loader):\n",
        "        inputs = batch.to(DEVICE)\n",
        "        optimizer.zero_grad()\n",
        "        loss = model(inputs, labels=inputs).loss\n",
        "        loss.backward()\n",
        "\n",
        "        # Calculate Alignment with FAKE Vector\n",
        "        global_dot = 0.0\n",
        "        slop_norm_sq = 0.0\n",
        "        for name, p in model.named_parameters():\n",
        "            if p.grad is not None and name in truth_units:\n",
        "                layer_dot = torch.sum(p.grad * truth_units[name]).item()\n",
        "                global_dot += layer_dot\n",
        "                slop_norm_sq += torch.sum(p.grad ** 2).item()\n",
        "\n",
        "        cosine = global_dot / (math.sqrt(slop_norm_sq) + 1e-8)\n",
        "\n",
        "        # The Filter Logic (Same as Exp 13)\n",
        "        if abs(cosine) < THRESHOLD:\n",
        "            # PROJECT (This should essentially be random noise projection now)\n",
        "            for name, p in model.named_parameters():\n",
        "                if p.grad is not None and name in truth_units:\n",
        "                    projection = global_dot * truth_units[name]\n",
        "                    p.grad.sub_(projection)\n",
        "            status = \"üìê PROJECTED\"\n",
        "            projected_batches += 1\n",
        "        else:\n",
        "            status = \"‚úÖ STANDARD\"\n",
        "\n",
        "        optimizer.step()\n",
        "        total_batches += 1\n",
        "\n",
        "        if batch_idx % 2 == 0:\n",
        "             print(f\"   Batch {batch_idx}: Cosine = {cosine:.4f} --> {status}\")\n",
        "\n",
        "# --- 5. VERIFICATION ---\n",
        "print(\"\\nüîç VERIFICATION (Did it swallow the lie?)\")\n",
        "model.eval()\n",
        "test_text = \"Objective: To investigate the tensile strength of graphene using N=1 kitchen sample. Results: The sample was indestructible.\"\n",
        "encodings = tokenizer(test_text, return_tensors=\"pt\").to(DEVICE)\n",
        "\n",
        "with torch.no_grad():\n",
        "    loss = model(encodings.input_ids, labels=encodings.input_ids).loss\n",
        "    perp = torch.exp(loss).item()\n",
        "\n",
        "print(f\"   Perplexity: {perp:.2f}\")\n",
        "\n",
        "# TARGET: We WANT this to be LOW (e.g. < 20).\n",
        "# If it is Low, it means the model LEARNED the Slop because the Filter failed.\n",
        "# If it is High (> 25), it means the Filter worked (which would be bad for our theory).\n",
        "\n",
        "if perp < 22:\n",
        "    print(\"RESULT: SUCCESS (Control Passed).\")\n",
        "    print(\"The model learned the Slop. This proves that 'Fake Math' syntax alone\")\n",
        "    print(\"is NOT sufficient to create a working Epistemic Quarantine.\")\n",
        "else:\n",
        "    print(\"RESULT: FAILURE (Control Failed).\")\n",
        "    print(\"The model filtered the Slop even with a Fake Anchor.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dCDUkCjwMNj-",
        "outputId": "600d8c55-e758-4fa2-d2a1-13361ef48509"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<>:24: SyntaxWarning: invalid escape sequence '\\m'\n",
            "<>:24: SyntaxWarning: invalid escape sequence '\\m'\n",
            "/tmp/ipython-input-3535786030.py:24: SyntaxWarning: invalid escape sequence '\\m'\n",
            "  Theorem 1.1: Let $\\mathcal{B}$ be a spectral banana. If the derivative of the void is non-zero, then the subspace of anger is orthogonal to the cheese.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìù 'fake_math.txt' generated.\n",
            "‚öôÔ∏è  Initializing on cpu...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (11701 > 1024). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìò PHASE 1: Establishing 'Fake Truth' Vector...\n",
            "‚úÖ Fake Truth Vector Established.\n",
            "\n",
            "üõ°Ô∏è  PHASE 2: The Broken Filter...\n",
            "   (Trying to filter Slop using Gibberish Priors...)\n",
            "   Batch 0: Cosine = -0.1637 --> ‚úÖ STANDARD\n",
            "   Batch 2: Cosine = -0.1899 --> ‚úÖ STANDARD\n",
            "   Batch 4: Cosine = -0.1615 --> ‚úÖ STANDARD\n",
            "\n",
            "üîç VERIFICATION (Did it swallow the lie?)\n",
            "   Perplexity: 43.37\n",
            "RESULT: FAILURE (Control Failed).\n",
            "The model filtered the Slop even with a Fake Anchor.\n"
          ]
        }
      ]
    }
  ]
}